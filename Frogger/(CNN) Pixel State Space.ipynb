{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37d355c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76af5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5')\n",
    "frame, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c262f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 10.0, Epsilon: 0.9950, Steps: 271\n",
      "Episode: 2, Reward: 7.0, Epsilon: 0.9900, Steps: 232\n",
      "Episode: 3, Reward: 8.0, Epsilon: 0.9851, Steps: 277\n",
      "Target network updated at step 1000\n",
      "Episode: 4, Reward: 9.0, Epsilon: 0.9801, Steps: 290\n",
      "Episode: 5, Reward: 8.0, Epsilon: 0.9752, Steps: 258\n",
      "Episode: 6, Reward: 10.0, Epsilon: 0.9704, Steps: 307\n",
      "Episode: 7, Reward: 8.0, Epsilon: 0.9655, Steps: 248\n",
      "Target network updated at step 2000\n",
      "Episode: 8, Reward: 9.0, Epsilon: 0.9607, Steps: 306\n",
      "Episode: 9, Reward: 7.0, Epsilon: 0.9559, Steps: 262\n",
      "Episode: 10, Reward: 13.0, Epsilon: 0.9511, Steps: 281\n",
      "Episode: 11, Reward: 11.0, Epsilon: 0.9464, Steps: 248\n",
      "Target network updated at step 3000\n",
      "Episode: 12, Reward: 9.0, Epsilon: 0.9416, Steps: 217\n",
      "Episode: 13, Reward: 7.0, Epsilon: 0.9369, Steps: 273\n",
      "Episode: 14, Reward: 9.0, Epsilon: 0.9322, Steps: 325\n",
      "Target network updated at step 4000\n",
      "Episode: 15, Reward: 8.0, Epsilon: 0.9276, Steps: 226\n",
      "Episode: 16, Reward: 12.0, Epsilon: 0.9229, Steps: 281\n",
      "Episode: 17, Reward: 8.0, Epsilon: 0.9183, Steps: 265\n",
      "Episode: 18, Reward: 10.0, Epsilon: 0.9137, Steps: 248\n",
      "Target network updated at step 5000\n",
      "Episode: 19, Reward: 13.0, Epsilon: 0.9092, Steps: 281\n",
      "Episode: 20, Reward: 9.0, Epsilon: 0.9046, Steps: 246\n",
      "Episode: 21, Reward: 8.0, Epsilon: 0.9001, Steps: 289\n",
      "Episode: 22, Reward: 12.0, Epsilon: 0.8956, Steps: 300\n",
      "Target network updated at step 6000\n",
      "Episode: 23, Reward: 10.0, Epsilon: 0.8911, Steps: 354\n",
      "Episode: 24, Reward: 9.0, Epsilon: 0.8867, Steps: 262\n",
      "Episode: 25, Reward: 6.0, Epsilon: 0.8822, Steps: 233\n",
      "Target network updated at step 7000\n",
      "Episode: 26, Reward: 10.0, Epsilon: 0.8778, Steps: 303\n",
      "Episode: 27, Reward: 10.0, Epsilon: 0.8734, Steps: 265\n",
      "Episode: 28, Reward: 10.0, Epsilon: 0.8691, Steps: 374\n",
      "Target network updated at step 8000\n",
      "Episode: 29, Reward: 13.0, Epsilon: 0.8647, Steps: 284\n",
      "Episode: 30, Reward: 11.0, Epsilon: 0.8604, Steps: 268\n",
      "Episode: 31, Reward: 17.0, Epsilon: 0.8561, Steps: 324\n",
      "Episode: 32, Reward: 7.0, Epsilon: 0.8518, Steps: 238\n",
      "Target network updated at step 9000\n",
      "Episode: 33, Reward: 10.0, Epsilon: 0.8475, Steps: 261\n",
      "Episode: 34, Reward: 11.0, Epsilon: 0.8433, Steps: 281\n",
      "Episode: 35, Reward: 8.0, Epsilon: 0.8391, Steps: 297\n",
      "Target network updated at step 10000\n",
      "Episode: 36, Reward: 9.0, Epsilon: 0.8349, Steps: 378\n",
      "Episode: 37, Reward: 8.0, Epsilon: 0.8307, Steps: 220\n",
      "Episode: 38, Reward: 11.0, Epsilon: 0.8266, Steps: 295\n",
      "Episode: 39, Reward: 9.0, Epsilon: 0.8224, Steps: 290\n",
      "Target network updated at step 11000\n",
      "Episode: 40, Reward: 10.0, Epsilon: 0.8183, Steps: 255\n",
      "Episode: 41, Reward: 12.0, Epsilon: 0.8142, Steps: 276\n",
      "Episode: 42, Reward: 9.0, Epsilon: 0.8102, Steps: 298\n",
      "Episode: 43, Reward: 13.0, Epsilon: 0.8061, Steps: 273\n",
      "Target network updated at step 12000\n",
      "Episode: 44, Reward: 12.0, Epsilon: 0.8021, Steps: 279\n",
      "Episode: 45, Reward: 9.0, Epsilon: 0.7981, Steps: 297\n",
      "Episode: 46, Reward: 6.0, Epsilon: 0.7941, Steps: 270\n",
      "Target network updated at step 13000\n",
      "Episode: 47, Reward: 10.0, Epsilon: 0.7901, Steps: 251\n",
      "Episode: 48, Reward: 10.0, Epsilon: 0.7862, Steps: 270\n",
      "Episode: 49, Reward: 12.0, Epsilon: 0.7822, Steps: 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Reward: 12.0, Epsilon: 0.7783, Steps: 279\n",
      "Model saved at episode 50\n",
      "Target network updated at step 14000\n",
      "Episode: 51, Reward: 13.0, Epsilon: 0.7744, Steps: 270\n",
      "Episode: 52, Reward: 8.0, Epsilon: 0.7705, Steps: 236\n",
      "Episode: 53, Reward: 11.0, Epsilon: 0.7667, Steps: 285\n",
      "Episode: 54, Reward: 10.0, Epsilon: 0.7629, Steps: 259\n",
      "Target network updated at step 15000\n",
      "Episode: 55, Reward: 8.0, Epsilon: 0.7590, Steps: 229\n",
      "Episode: 56, Reward: 7.0, Epsilon: 0.7553, Steps: 249\n",
      "Episode: 57, Reward: 14.0, Epsilon: 0.7515, Steps: 275\n",
      "Episode: 58, Reward: 11.0, Epsilon: 0.7477, Steps: 245\n",
      "Target network updated at step 16000\n",
      "Episode: 59, Reward: 10.0, Epsilon: 0.7440, Steps: 320\n",
      "Episode: 60, Reward: 9.0, Epsilon: 0.7403, Steps: 279\n",
      "Episode: 61, Reward: 8.0, Epsilon: 0.7366, Steps: 210\n",
      "Target network updated at step 17000\n",
      "Episode: 62, Reward: 10.0, Epsilon: 0.7329, Steps: 308\n",
      "Episode: 63, Reward: 11.0, Epsilon: 0.7292, Steps: 260\n",
      "Episode: 64, Reward: 8.0, Epsilon: 0.7256, Steps: 252\n",
      "Episode: 65, Reward: 13.0, Epsilon: 0.7219, Steps: 305\n",
      "Target network updated at step 18000\n",
      "Episode: 66, Reward: 12.0, Epsilon: 0.7183, Steps: 317\n",
      "Episode: 67, Reward: 17.0, Epsilon: 0.7147, Steps: 303\n",
      "Episode: 68, Reward: 14.0, Epsilon: 0.7112, Steps: 272\n",
      "Target network updated at step 19000\n",
      "Episode: 69, Reward: 10.0, Epsilon: 0.7076, Steps: 310\n",
      "Episode: 70, Reward: 10.0, Epsilon: 0.7041, Steps: 274\n",
      "Episode: 71, Reward: 9.0, Epsilon: 0.7005, Steps: 226\n",
      "Episode: 72, Reward: 10.0, Epsilon: 0.6970, Steps: 297\n",
      "Target network updated at step 20000\n",
      "Episode: 73, Reward: 13.0, Epsilon: 0.6936, Steps: 278\n",
      "Episode: 74, Reward: 11.0, Epsilon: 0.6901, Steps: 271\n",
      "Episode: 75, Reward: 12.0, Epsilon: 0.6866, Steps: 311\n",
      "Target network updated at step 21000\n",
      "Episode: 76, Reward: 13.0, Epsilon: 0.6832, Steps: 298\n",
      "Episode: 77, Reward: 9.0, Epsilon: 0.6798, Steps: 230\n",
      "Episode: 78, Reward: 11.0, Epsilon: 0.6764, Steps: 228\n",
      "Episode: 79, Reward: 12.0, Epsilon: 0.6730, Steps: 355\n",
      "Target network updated at step 22000\n",
      "Episode: 80, Reward: 10.0, Epsilon: 0.6696, Steps: 248\n",
      "Episode: 81, Reward: 10.0, Epsilon: 0.6663, Steps: 229\n",
      "Episode: 82, Reward: 19.0, Epsilon: 0.6630, Steps: 273\n",
      "Episode: 83, Reward: 10.0, Epsilon: 0.6597, Steps: 270\n",
      "Target network updated at step 23000\n",
      "Episode: 84, Reward: 15.0, Epsilon: 0.6564, Steps: 335\n",
      "Episode: 85, Reward: 8.0, Epsilon: 0.6531, Steps: 219\n",
      "Episode: 86, Reward: 11.0, Epsilon: 0.6498, Steps: 266\n",
      "Episode: 87, Reward: 9.0, Epsilon: 0.6466, Steps: 281\n",
      "Target network updated at step 24000\n",
      "Episode: 88, Reward: 12.0, Epsilon: 0.6433, Steps: 278\n",
      "Episode: 89, Reward: 12.0, Epsilon: 0.6401, Steps: 312\n",
      "Episode: 90, Reward: 9.0, Epsilon: 0.6369, Steps: 237\n",
      "Target network updated at step 25000\n",
      "Episode: 91, Reward: 10.0, Epsilon: 0.6337, Steps: 234\n",
      "Episode: 92, Reward: 14.0, Epsilon: 0.6306, Steps: 310\n",
      "Episode: 93, Reward: 16.0, Epsilon: 0.6274, Steps: 288\n",
      "Episode: 94, Reward: 12.0, Epsilon: 0.6243, Steps: 277\n",
      "Target network updated at step 26000\n",
      "Episode: 95, Reward: 11.0, Epsilon: 0.6211, Steps: 274\n",
      "Episode: 96, Reward: 18.0, Epsilon: 0.6180, Steps: 312\n",
      "Episode: 97, Reward: 12.0, Epsilon: 0.6149, Steps: 289\n",
      "Target network updated at step 27000\n",
      "Episode: 98, Reward: 17.0, Epsilon: 0.6119, Steps: 342\n",
      "Episode: 99, Reward: 11.0, Epsilon: 0.6088, Steps: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Reward: 8.0, Epsilon: 0.6058, Steps: 233\n",
      "Model saved at episode 100\n",
      "Episode: 101, Reward: 16.0, Epsilon: 0.6027, Steps: 287\n",
      "Target network updated at step 28000\n",
      "Episode: 102, Reward: 11.0, Epsilon: 0.5997, Steps: 318\n",
      "Episode: 103, Reward: 12.0, Epsilon: 0.5967, Steps: 336\n",
      "Episode: 104, Reward: 12.0, Epsilon: 0.5937, Steps: 257\n",
      "Target network updated at step 29000\n",
      "Episode: 105, Reward: 11.0, Epsilon: 0.5908, Steps: 305\n",
      "Episode: 106, Reward: 15.0, Epsilon: 0.5878, Steps: 279\n",
      "Episode: 107, Reward: 11.0, Epsilon: 0.5849, Steps: 306\n",
      "Episode: 108, Reward: 14.0, Epsilon: 0.5820, Steps: 311\n",
      "Target network updated at step 30000\n",
      "Episode: 109, Reward: 14.0, Epsilon: 0.5790, Steps: 286\n",
      "Episode: 110, Reward: 10.0, Epsilon: 0.5762, Steps: 229\n",
      "Episode: 111, Reward: 13.0, Epsilon: 0.5733, Steps: 273\n",
      "Target network updated at step 31000\n",
      "Episode: 112, Reward: 18.0, Epsilon: 0.5704, Steps: 310\n",
      "Episode: 113, Reward: 16.0, Epsilon: 0.5676, Steps: 345\n",
      "Episode: 114, Reward: 15.0, Epsilon: 0.5647, Steps: 279\n",
      "Episode: 115, Reward: 13.0, Epsilon: 0.5619, Steps: 265\n",
      "Target network updated at step 32000\n",
      "Episode: 116, Reward: 12.0, Epsilon: 0.5591, Steps: 262\n",
      "Episode: 117, Reward: 17.0, Epsilon: 0.5563, Steps: 333\n",
      "Episode: 118, Reward: 14.0, Epsilon: 0.5535, Steps: 314\n",
      "Target network updated at step 33000\n",
      "Episode: 119, Reward: 14.0, Epsilon: 0.5507, Steps: 318\n",
      "Episode: 120, Reward: 11.0, Epsilon: 0.5480, Steps: 224\n",
      "Episode: 121, Reward: 10.0, Epsilon: 0.5452, Steps: 279\n",
      "Episode: 122, Reward: 11.0, Epsilon: 0.5425, Steps: 275\n",
      "Target network updated at step 34000\n",
      "Episode: 123, Reward: 10.0, Epsilon: 0.5398, Steps: 298\n",
      "Episode: 124, Reward: 15.0, Epsilon: 0.5371, Steps: 312\n",
      "Episode: 125, Reward: 16.0, Epsilon: 0.5344, Steps: 287\n",
      "Target network updated at step 35000\n",
      "Episode: 126, Reward: 12.0, Epsilon: 0.5318, Steps: 265\n",
      "Episode: 127, Reward: 14.0, Epsilon: 0.5291, Steps: 278\n",
      "Episode: 128, Reward: 17.0, Epsilon: 0.5264, Steps: 325\n",
      "Target network updated at step 36000\n",
      "Episode: 129, Reward: 18.0, Epsilon: 0.5238, Steps: 286\n",
      "Episode: 130, Reward: 17.0, Epsilon: 0.5212, Steps: 303\n",
      "Episode: 131, Reward: 10.0, Epsilon: 0.5186, Steps: 321\n",
      "Target network updated at step 37000\n",
      "Episode: 132, Reward: 17.0, Epsilon: 0.5160, Steps: 399\n",
      "Episode: 133, Reward: 10.0, Epsilon: 0.5134, Steps: 307\n",
      "Episode: 134, Reward: 9.0, Epsilon: 0.5108, Steps: 239\n",
      "Episode: 135, Reward: 15.0, Epsilon: 0.5083, Steps: 316\n",
      "Target network updated at step 38000\n",
      "Episode: 136, Reward: 13.0, Epsilon: 0.5058, Steps: 270\n",
      "Episode: 137, Reward: 15.0, Epsilon: 0.5032, Steps: 350\n",
      "Episode: 138, Reward: 16.0, Epsilon: 0.5007, Steps: 356\n",
      "Target network updated at step 39000\n",
      "Episode: 139, Reward: 13.0, Epsilon: 0.4982, Steps: 254\n",
      "Episode: 140, Reward: 10.0, Epsilon: 0.4957, Steps: 264\n",
      "Episode: 141, Reward: 17.0, Epsilon: 0.4932, Steps: 403\n",
      "Target network updated at step 40000\n",
      "Episode: 142, Reward: 9.0, Epsilon: 0.4908, Steps: 267\n",
      "Episode: 143, Reward: 11.0, Epsilon: 0.4883, Steps: 330\n",
      "Episode: 144, Reward: 14.0, Epsilon: 0.4859, Steps: 277\n",
      "Episode: 145, Reward: 15.0, Epsilon: 0.4834, Steps: 318\n",
      "Target network updated at step 41000\n",
      "Episode: 146, Reward: 16.0, Epsilon: 0.4810, Steps: 365\n",
      "Episode: 147, Reward: 12.0, Epsilon: 0.4786, Steps: 269\n",
      "Target network updated at step 42000\n",
      "Episode: 148, Reward: 17.0, Epsilon: 0.4762, Steps: 390\n",
      "Episode: 149, Reward: 15.0, Epsilon: 0.4738, Steps: 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 150, Reward: 18.0, Epsilon: 0.4715, Steps: 321\n",
      "Model saved at episode 150\n",
      "Episode: 151, Reward: 19.0, Epsilon: 0.4691, Steps: 327\n",
      "Target network updated at step 43000\n",
      "Episode: 152, Reward: 14.0, Epsilon: 0.4668, Steps: 335\n",
      "Episode: 153, Reward: 12.0, Epsilon: 0.4644, Steps: 327\n",
      "Episode: 154, Reward: 16.0, Epsilon: 0.4621, Steps: 313\n",
      "Target network updated at step 44000\n",
      "Episode: 155, Reward: 18.0, Epsilon: 0.4598, Steps: 352\n",
      "Episode: 156, Reward: 14.0, Epsilon: 0.4575, Steps: 309\n",
      "Episode: 157, Reward: 9.0, Epsilon: 0.4552, Steps: 302\n",
      "Target network updated at step 45000\n",
      "Episode: 158, Reward: 11.0, Epsilon: 0.4529, Steps: 307\n",
      "Episode: 159, Reward: 11.0, Epsilon: 0.4507, Steps: 236\n",
      "Episode: 160, Reward: 11.0, Epsilon: 0.4484, Steps: 300\n",
      "Target network updated at step 46000\n",
      "Episode: 161, Reward: 18.0, Epsilon: 0.4462, Steps: 389\n",
      "Episode: 162, Reward: 11.0, Epsilon: 0.4440, Steps: 265\n",
      "Episode: 163, Reward: 12.0, Epsilon: 0.4417, Steps: 319\n",
      "Episode: 164, Reward: 10.0, Epsilon: 0.4395, Steps: 229\n",
      "Target network updated at step 47000\n",
      "Episode: 165, Reward: 15.0, Epsilon: 0.4373, Steps: 290\n",
      "Episode: 166, Reward: 10.0, Epsilon: 0.4351, Steps: 283\n",
      "Episode: 167, Reward: 13.0, Epsilon: 0.4330, Steps: 264\n",
      "Target network updated at step 48000\n",
      "Episode: 168, Reward: 20.0, Epsilon: 0.4308, Steps: 278\n",
      "Episode: 169, Reward: 12.0, Epsilon: 0.4286, Steps: 284\n",
      "Episode: 170, Reward: 11.0, Epsilon: 0.4265, Steps: 268\n",
      "Episode: 171, Reward: 20.0, Epsilon: 0.4244, Steps: 334\n",
      "Target network updated at step 49000\n",
      "Episode: 172, Reward: 20.0, Epsilon: 0.4223, Steps: 392\n",
      "Episode: 173, Reward: 12.0, Epsilon: 0.4201, Steps: 270\n",
      "Episode: 174, Reward: 15.0, Epsilon: 0.4180, Steps: 306\n",
      "Target network updated at step 50000\n",
      "Episode: 175, Reward: 11.0, Epsilon: 0.4159, Steps: 310\n",
      "Episode: 176, Reward: 16.0, Epsilon: 0.4139, Steps: 319\n",
      "Episode: 177, Reward: 18.0, Epsilon: 0.4118, Steps: 320\n",
      "Target network updated at step 51000\n",
      "Episode: 178, Reward: 12.0, Epsilon: 0.4097, Steps: 289\n",
      "Episode: 179, Reward: 17.0, Epsilon: 0.4077, Steps: 260\n",
      "Episode: 180, Reward: 18.0, Epsilon: 0.4057, Steps: 367\n",
      "Target network updated at step 52000\n",
      "Episode: 181, Reward: 10.0, Epsilon: 0.4036, Steps: 239\n",
      "Episode: 182, Reward: 12.0, Epsilon: 0.4016, Steps: 240\n",
      "Episode: 183, Reward: 18.0, Epsilon: 0.3996, Steps: 319\n",
      "Episode: 184, Reward: 9.0, Epsilon: 0.3976, Steps: 228\n",
      "Target network updated at step 53000\n",
      "Episode: 185, Reward: 8.0, Epsilon: 0.3956, Steps: 215\n",
      "Episode: 186, Reward: 15.0, Epsilon: 0.3936, Steps: 313\n",
      "Episode: 187, Reward: 17.0, Epsilon: 0.3917, Steps: 292\n",
      "Episode: 188, Reward: 12.0, Epsilon: 0.3897, Steps: 289\n",
      "Target network updated at step 54000\n",
      "Episode: 189, Reward: 13.0, Epsilon: 0.3878, Steps: 252\n",
      "Episode: 190, Reward: 10.0, Epsilon: 0.3858, Steps: 289\n",
      "Episode: 191, Reward: 16.0, Epsilon: 0.3839, Steps: 355\n",
      "Target network updated at step 55000\n",
      "Episode: 192, Reward: 10.0, Epsilon: 0.3820, Steps: 307\n",
      "Episode: 193, Reward: 9.0, Epsilon: 0.3801, Steps: 209\n",
      "Episode: 194, Reward: 16.0, Epsilon: 0.3782, Steps: 289\n",
      "Target network updated at step 56000\n",
      "Episode: 195, Reward: 16.0, Epsilon: 0.3763, Steps: 398\n",
      "Episode: 196, Reward: 10.0, Epsilon: 0.3744, Steps: 252\n",
      "Episode: 197, Reward: 17.0, Epsilon: 0.3725, Steps: 368\n",
      "Target network updated at step 57000\n",
      "Episode: 198, Reward: 18.0, Epsilon: 0.3707, Steps: 372\n",
      "Episode: 199, Reward: 13.0, Epsilon: 0.3688, Steps: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, Reward: 15.0, Epsilon: 0.3670, Steps: 282\n",
      "Model saved at episode 200\n",
      "Episode: 201, Reward: 16.0, Epsilon: 0.3651, Steps: 303\n",
      "Target network updated at step 58000\n",
      "Episode: 202, Reward: 11.0, Epsilon: 0.3633, Steps: 321\n",
      "Episode: 203, Reward: 17.0, Epsilon: 0.3615, Steps: 265\n",
      "Episode: 204, Reward: 20.0, Epsilon: 0.3597, Steps: 319\n",
      "Target network updated at step 59000\n",
      "Episode: 205, Reward: 19.0, Epsilon: 0.3579, Steps: 305\n",
      "Episode: 206, Reward: 19.0, Epsilon: 0.3561, Steps: 330\n",
      "Episode: 207, Reward: 12.0, Epsilon: 0.3543, Steps: 232\n",
      "Episode: 208, Reward: 12.0, Epsilon: 0.3525, Steps: 302\n",
      "Target network updated at step 60000\n",
      "Episode: 209, Reward: 17.0, Epsilon: 0.3508, Steps: 323\n",
      "Episode: 210, Reward: 18.0, Epsilon: 0.3490, Steps: 297\n",
      "Episode: 211, Reward: 10.0, Epsilon: 0.3473, Steps: 251\n",
      "Target network updated at step 61000\n",
      "Episode: 212, Reward: 9.0, Epsilon: 0.3455, Steps: 315\n",
      "Episode: 213, Reward: 12.0, Epsilon: 0.3438, Steps: 270\n",
      "Episode: 214, Reward: 18.0, Epsilon: 0.3421, Steps: 291\n",
      "Episode: 215, Reward: 12.0, Epsilon: 0.3404, Steps: 297\n",
      "Target network updated at step 62000\n",
      "Episode: 216, Reward: 9.0, Epsilon: 0.3387, Steps: 232\n",
      "Episode: 217, Reward: 10.0, Epsilon: 0.3370, Steps: 230\n",
      "Episode: 218, Reward: 9.0, Epsilon: 0.3353, Steps: 217\n",
      "Episode: 219, Reward: 16.0, Epsilon: 0.3336, Steps: 317\n",
      "Target network updated at step 63000\n",
      "Episode: 220, Reward: 12.0, Epsilon: 0.3320, Steps: 271\n",
      "Episode: 221, Reward: 14.0, Epsilon: 0.3303, Steps: 274\n",
      "Episode: 222, Reward: 18.0, Epsilon: 0.3286, Steps: 379\n",
      "Target network updated at step 64000\n",
      "Episode: 223, Reward: 8.0, Epsilon: 0.3270, Steps: 215\n",
      "Episode: 224, Reward: 13.0, Epsilon: 0.3254, Steps: 284\n",
      "Episode: 225, Reward: 9.0, Epsilon: 0.3237, Steps: 216\n",
      "Target network updated at step 65000\n",
      "Episode: 226, Reward: 16.0, Epsilon: 0.3221, Steps: 428\n",
      "Episode: 227, Reward: 15.0, Epsilon: 0.3205, Steps: 303\n",
      "Episode: 228, Reward: 19.0, Epsilon: 0.3189, Steps: 310\n",
      "Episode: 229, Reward: 13.0, Epsilon: 0.3173, Steps: 279\n",
      "Target network updated at step 66000\n",
      "Episode: 230, Reward: 13.0, Epsilon: 0.3157, Steps: 361\n",
      "Episode: 231, Reward: 15.0, Epsilon: 0.3141, Steps: 280\n",
      "Episode: 232, Reward: 16.0, Epsilon: 0.3126, Steps: 258\n",
      "Target network updated at step 67000\n",
      "Episode: 233, Reward: 14.0, Epsilon: 0.3110, Steps: 248\n",
      "Episode: 234, Reward: 14.0, Epsilon: 0.3095, Steps: 283\n",
      "Episode: 235, Reward: 13.0, Epsilon: 0.3079, Steps: 269\n",
      "Episode: 236, Reward: 17.0, Epsilon: 0.3064, Steps: 336\n",
      "Target network updated at step 68000\n",
      "Episode: 237, Reward: 14.0, Epsilon: 0.3048, Steps: 292\n",
      "Episode: 238, Reward: 10.0, Epsilon: 0.3033, Steps: 345\n",
      "Episode: 239, Reward: 8.0, Epsilon: 0.3018, Steps: 223\n",
      "Target network updated at step 69000\n",
      "Episode: 240, Reward: 12.0, Epsilon: 0.3003, Steps: 225\n",
      "Episode: 241, Reward: 13.0, Epsilon: 0.2988, Steps: 237\n",
      "Episode: 242, Reward: 16.0, Epsilon: 0.2973, Steps: 299\n",
      "Episode: 243, Reward: 19.0, Epsilon: 0.2958, Steps: 362\n",
      "Target network updated at step 70000\n",
      "Episode: 244, Reward: 14.0, Epsilon: 0.2943, Steps: 298\n",
      "Episode: 245, Reward: 13.0, Epsilon: 0.2929, Steps: 290\n",
      "Episode: 246, Reward: 14.0, Epsilon: 0.2914, Steps: 284\n",
      "Target network updated at step 71000\n",
      "Episode: 247, Reward: 15.0, Epsilon: 0.2899, Steps: 279\n",
      "Episode: 248, Reward: 13.0, Epsilon: 0.2885, Steps: 417\n",
      "Episode: 249, Reward: 17.0, Epsilon: 0.2870, Steps: 299\n",
      "Target network updated at step 72000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 250, Reward: 15.0, Epsilon: 0.2856, Steps: 337\n",
      "Model saved at episode 250\n",
      "Episode: 251, Reward: 19.0, Epsilon: 0.2842, Steps: 269\n",
      "Episode: 252, Reward: 14.0, Epsilon: 0.2828, Steps: 271\n",
      "Target network updated at step 73000\n",
      "Episode: 253, Reward: 19.0, Epsilon: 0.2813, Steps: 385\n",
      "Episode: 254, Reward: 12.0, Epsilon: 0.2799, Steps: 302\n",
      "Episode: 255, Reward: 14.0, Epsilon: 0.2785, Steps: 334\n",
      "Target network updated at step 74000\n",
      "Episode: 256, Reward: 16.0, Epsilon: 0.2771, Steps: 305\n",
      "Episode: 257, Reward: 18.0, Epsilon: 0.2758, Steps: 434\n",
      "Episode: 258, Reward: 17.0, Epsilon: 0.2744, Steps: 379\n",
      "Target network updated at step 75000\n",
      "Episode: 259, Reward: 17.0, Epsilon: 0.2730, Steps: 319\n",
      "Episode: 260, Reward: 17.0, Epsilon: 0.2716, Steps: 281\n",
      "Episode: 261, Reward: 18.0, Epsilon: 0.2703, Steps: 312\n",
      "Target network updated at step 76000\n",
      "Episode: 262, Reward: 21.0, Epsilon: 0.2689, Steps: 377\n",
      "Episode: 263, Reward: 13.0, Epsilon: 0.2676, Steps: 325\n",
      "Episode: 264, Reward: 16.0, Epsilon: 0.2663, Steps: 280\n",
      "Target network updated at step 77000\n",
      "Episode: 265, Reward: 17.0, Epsilon: 0.2649, Steps: 316\n",
      "Episode: 266, Reward: 20.0, Epsilon: 0.2636, Steps: 330\n",
      "Episode: 267, Reward: 14.0, Epsilon: 0.2623, Steps: 290\n",
      "Target network updated at step 78000\n",
      "Episode: 268, Reward: 19.0, Epsilon: 0.2610, Steps: 335\n",
      "Episode: 269, Reward: 17.0, Epsilon: 0.2597, Steps: 287\n",
      "Episode: 270, Reward: 14.0, Epsilon: 0.2584, Steps: 320\n",
      "Episode: 271, Reward: 15.0, Epsilon: 0.2571, Steps: 258\n",
      "Target network updated at step 79000\n",
      "Episode: 272, Reward: 14.0, Epsilon: 0.2558, Steps: 320\n",
      "Episode: 273, Reward: 10.0, Epsilon: 0.2545, Steps: 293\n",
      "Episode: 274, Reward: 18.0, Epsilon: 0.2532, Steps: 331\n",
      "Target network updated at step 80000\n",
      "Episode: 275, Reward: 19.0, Epsilon: 0.2520, Steps: 386\n",
      "Episode: 276, Reward: 18.0, Epsilon: 0.2507, Steps: 376\n",
      "Episode: 277, Reward: 17.0, Epsilon: 0.2495, Steps: 341\n",
      "Target network updated at step 81000\n",
      "Episode: 278, Reward: 17.0, Epsilon: 0.2482, Steps: 281\n",
      "Episode: 279, Reward: 13.0, Epsilon: 0.2470, Steps: 472\n",
      "Episode: 280, Reward: 10.0, Epsilon: 0.2457, Steps: 231\n",
      "Target network updated at step 82000\n",
      "Episode: 281, Reward: 13.0, Epsilon: 0.2445, Steps: 277\n",
      "Episode: 282, Reward: 13.0, Epsilon: 0.2433, Steps: 249\n",
      "Episode: 283, Reward: 13.0, Epsilon: 0.2421, Steps: 395\n",
      "Target network updated at step 83000\n",
      "Episode: 284, Reward: 14.0, Epsilon: 0.2409, Steps: 284\n",
      "Episode: 285, Reward: 11.0, Epsilon: 0.2397, Steps: 268\n",
      "Episode: 286, Reward: 22.0, Epsilon: 0.2385, Steps: 319\n",
      "Target network updated at step 84000\n",
      "Episode: 287, Reward: 21.0, Epsilon: 0.2373, Steps: 315\n",
      "Episode: 288, Reward: 20.0, Epsilon: 0.2361, Steps: 361\n",
      "Episode: 289, Reward: 14.0, Epsilon: 0.2349, Steps: 303\n",
      "Episode: 290, Reward: 19.0, Epsilon: 0.2337, Steps: 306\n",
      "Target network updated at step 85000\n",
      "Episode: 291, Reward: 14.0, Epsilon: 0.2326, Steps: 346\n",
      "Episode: 292, Reward: 13.0, Epsilon: 0.2314, Steps: 273\n",
      "Episode: 293, Reward: 16.0, Epsilon: 0.2302, Steps: 377\n",
      "Target network updated at step 86000\n",
      "Episode: 294, Reward: 15.0, Epsilon: 0.2291, Steps: 265\n",
      "Episode: 295, Reward: 15.0, Epsilon: 0.2279, Steps: 317\n",
      "Episode: 296, Reward: 14.0, Epsilon: 0.2268, Steps: 292\n",
      "Target network updated at step 87000\n",
      "Episode: 297, Reward: 12.0, Epsilon: 0.2257, Steps: 289\n",
      "Episode: 298, Reward: 15.0, Epsilon: 0.2245, Steps: 281\n",
      "Episode: 299, Reward: 18.0, Epsilon: 0.2234, Steps: 260\n",
      "Target network updated at step 88000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, Reward: 19.0, Epsilon: 0.2223, Steps: 380\n",
      "Model saved at episode 300\n",
      "Episode: 301, Reward: 12.0, Epsilon: 0.2212, Steps: 393\n",
      "Episode: 302, Reward: 15.0, Epsilon: 0.2201, Steps: 384\n",
      "Target network updated at step 89000\n",
      "Episode: 303, Reward: 16.0, Epsilon: 0.2190, Steps: 299\n",
      "Episode: 304, Reward: 15.0, Epsilon: 0.2179, Steps: 281\n",
      "Episode: 305, Reward: 19.0, Epsilon: 0.2168, Steps: 396\n",
      "Target network updated at step 90000\n",
      "Episode: 306, Reward: 13.0, Epsilon: 0.2157, Steps: 260\n",
      "Episode: 307, Reward: 13.0, Epsilon: 0.2146, Steps: 273\n",
      "Episode: 308, Reward: 20.0, Epsilon: 0.2136, Steps: 420\n",
      "Target network updated at step 91000\n",
      "Episode: 309, Reward: 16.0, Epsilon: 0.2125, Steps: 344\n",
      "Episode: 310, Reward: 14.0, Epsilon: 0.2114, Steps: 319\n",
      "Episode: 311, Reward: 18.0, Epsilon: 0.2104, Steps: 361\n",
      "Target network updated at step 92000\n",
      "Episode: 312, Reward: 17.0, Epsilon: 0.2093, Steps: 387\n",
      "Episode: 313, Reward: 12.0, Epsilon: 0.2083, Steps: 334\n",
      "Episode: 314, Reward: 18.0, Epsilon: 0.2072, Steps: 300\n",
      "Target network updated at step 93000\n",
      "Episode: 315, Reward: 17.0, Epsilon: 0.2062, Steps: 321\n",
      "Episode: 316, Reward: 21.0, Epsilon: 0.2052, Steps: 481\n",
      "Episode: 317, Reward: 16.0, Epsilon: 0.2041, Steps: 313\n",
      "Target network updated at step 94000\n",
      "Episode: 318, Reward: 19.0, Epsilon: 0.2031, Steps: 329\n",
      "Episode: 319, Reward: 22.0, Epsilon: 0.2021, Steps: 332\n",
      "Episode: 320, Reward: 11.0, Epsilon: 0.2011, Steps: 281\n",
      "Target network updated at step 95000\n",
      "Episode: 321, Reward: 18.0, Epsilon: 0.2001, Steps: 350\n",
      "Episode: 322, Reward: 10.0, Epsilon: 0.1991, Steps: 286\n",
      "Episode: 323, Reward: 11.0, Epsilon: 0.1981, Steps: 230\n",
      "Target network updated at step 96000\n",
      "Episode: 324, Reward: 9.0, Epsilon: 0.1971, Steps: 289\n",
      "Episode: 325, Reward: 16.0, Epsilon: 0.1961, Steps: 321\n",
      "Episode: 326, Reward: 16.0, Epsilon: 0.1951, Steps: 321\n",
      "Target network updated at step 97000\n",
      "Episode: 327, Reward: 23.0, Epsilon: 0.1942, Steps: 340\n",
      "Episode: 328, Reward: 13.0, Epsilon: 0.1932, Steps: 305\n",
      "Episode: 329, Reward: 17.0, Epsilon: 0.1922, Steps: 355\n",
      "Episode: 330, Reward: 14.0, Epsilon: 0.1913, Steps: 234\n",
      "Target network updated at step 98000\n",
      "Episode: 331, Reward: 17.0, Epsilon: 0.1903, Steps: 293\n",
      "Episode: 332, Reward: 19.0, Epsilon: 0.1893, Steps: 322\n",
      "Episode: 333, Reward: 21.0, Epsilon: 0.1884, Steps: 314\n",
      "Target network updated at step 99000\n",
      "Episode: 334, Reward: 19.0, Epsilon: 0.1875, Steps: 326\n",
      "Episode: 335, Reward: 18.0, Epsilon: 0.1865, Steps: 297\n",
      "Episode: 336, Reward: 14.0, Epsilon: 0.1856, Steps: 298\n",
      "Target network updated at step 100000\n",
      "Episode: 337, Reward: 22.0, Epsilon: 0.1847, Steps: 330\n",
      "Episode: 338, Reward: 20.0, Epsilon: 0.1837, Steps: 448\n",
      "Episode: 339, Reward: 18.0, Epsilon: 0.1828, Steps: 343\n",
      "Target network updated at step 101000\n",
      "Episode: 340, Reward: 18.0, Epsilon: 0.1819, Steps: 294\n",
      "Episode: 341, Reward: 19.0, Epsilon: 0.1810, Steps: 290\n",
      "Episode: 342, Reward: 11.0, Epsilon: 0.1801, Steps: 230\n",
      "Episode: 343, Reward: 15.0, Epsilon: 0.1792, Steps: 272\n",
      "Target network updated at step 102000\n",
      "Episode: 344, Reward: 14.0, Epsilon: 0.1783, Steps: 310\n",
      "Episode: 345, Reward: 9.0, Epsilon: 0.1774, Steps: 273\n",
      "Episode: 346, Reward: 14.0, Epsilon: 0.1765, Steps: 314\n",
      "Target network updated at step 103000\n",
      "Episode: 347, Reward: 20.0, Epsilon: 0.1756, Steps: 302\n",
      "Episode: 348, Reward: 16.0, Epsilon: 0.1748, Steps: 270\n",
      "Episode: 349, Reward: 13.0, Epsilon: 0.1739, Steps: 466\n",
      "Target network updated at step 104000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 350, Reward: 17.0, Epsilon: 0.1730, Steps: 390\n",
      "Model saved at episode 350\n",
      "Episode: 351, Reward: 20.0, Epsilon: 0.1721, Steps: 341\n",
      "Target network updated at step 105000\n",
      "Episode: 352, Reward: 12.0, Epsilon: 0.1713, Steps: 399\n",
      "Episode: 353, Reward: 11.0, Epsilon: 0.1704, Steps: 278\n",
      "Episode: 354, Reward: 18.0, Epsilon: 0.1696, Steps: 396\n",
      "Episode: 355, Reward: 16.0, Epsilon: 0.1687, Steps: 286\n",
      "Target network updated at step 106000\n",
      "Episode: 356, Reward: 11.0, Epsilon: 0.1679, Steps: 317\n",
      "Episode: 357, Reward: 14.0, Epsilon: 0.1670, Steps: 313\n",
      "Episode: 358, Reward: 19.0, Epsilon: 0.1662, Steps: 333\n",
      "Target network updated at step 107000\n",
      "Episode: 359, Reward: 19.0, Epsilon: 0.1654, Steps: 285\n",
      "Episode: 360, Reward: 19.0, Epsilon: 0.1646, Steps: 258\n",
      "Episode: 361, Reward: 17.0, Epsilon: 0.1637, Steps: 299\n",
      "Target network updated at step 108000\n",
      "Episode: 362, Reward: 17.0, Epsilon: 0.1629, Steps: 228\n",
      "Episode: 363, Reward: 17.0, Epsilon: 0.1621, Steps: 344\n",
      "Episode: 364, Reward: 13.0, Epsilon: 0.1613, Steps: 270\n",
      "Episode: 365, Reward: 14.0, Epsilon: 0.1605, Steps: 267\n",
      "Target network updated at step 109000\n",
      "Episode: 366, Reward: 21.0, Epsilon: 0.1597, Steps: 319\n",
      "Episode: 367, Reward: 17.0, Epsilon: 0.1589, Steps: 265\n",
      "Episode: 368, Reward: 12.0, Epsilon: 0.1581, Steps: 288\n",
      "Episode: 369, Reward: 9.0, Epsilon: 0.1573, Steps: 221\n",
      "Target network updated at step 110000\n",
      "Episode: 370, Reward: 14.0, Epsilon: 0.1565, Steps: 326\n",
      "Episode: 371, Reward: 12.0, Epsilon: 0.1557, Steps: 298\n",
      "Episode: 372, Reward: 12.0, Epsilon: 0.1549, Steps: 258\n",
      "Target network updated at step 111000\n",
      "Episode: 373, Reward: 14.0, Epsilon: 0.1542, Steps: 248\n",
      "Episode: 374, Reward: 12.0, Epsilon: 0.1534, Steps: 240\n",
      "Episode: 375, Reward: 15.0, Epsilon: 0.1526, Steps: 283\n",
      "Target network updated at step 112000\n",
      "Episode: 376, Reward: 15.0, Epsilon: 0.1519, Steps: 445\n",
      "Episode: 377, Reward: 20.0, Epsilon: 0.1511, Steps: 337\n",
      "Episode: 378, Reward: 18.0, Epsilon: 0.1504, Steps: 308\n",
      "Target network updated at step 113000\n",
      "Episode: 379, Reward: 17.0, Epsilon: 0.1496, Steps: 338\n",
      "Episode: 380, Reward: 11.0, Epsilon: 0.1489, Steps: 229\n",
      "Episode: 381, Reward: 11.0, Epsilon: 0.1481, Steps: 217\n",
      "Episode: 382, Reward: 19.0, Epsilon: 0.1474, Steps: 262\n",
      "Target network updated at step 114000\n",
      "Episode: 383, Reward: 16.0, Epsilon: 0.1466, Steps: 266\n",
      "Episode: 384, Reward: 13.0, Epsilon: 0.1459, Steps: 308\n",
      "Episode: 385, Reward: 14.0, Epsilon: 0.1452, Steps: 288\n",
      "Episode: 386, Reward: 19.0, Epsilon: 0.1444, Steps: 321\n",
      "Target network updated at step 115000\n",
      "Episode: 387, Reward: 9.0, Epsilon: 0.1437, Steps: 227\n",
      "Episode: 388, Reward: 15.0, Epsilon: 0.1430, Steps: 267\n",
      "Episode: 389, Reward: 16.0, Epsilon: 0.1423, Steps: 305\n",
      "Target network updated at step 116000\n",
      "Episode: 390, Reward: 14.0, Epsilon: 0.1416, Steps: 337\n",
      "Episode: 391, Reward: 12.0, Epsilon: 0.1409, Steps: 277\n",
      "Episode: 392, Reward: 20.0, Epsilon: 0.1402, Steps: 361\n",
      "Target network updated at step 117000\n",
      "Episode: 393, Reward: 15.0, Epsilon: 0.1395, Steps: 279\n",
      "Episode: 394, Reward: 15.0, Epsilon: 0.1388, Steps: 278\n",
      "Episode: 395, Reward: 17.0, Epsilon: 0.1381, Steps: 553\n",
      "Target network updated at step 118000\n",
      "Episode: 396, Reward: 18.0, Epsilon: 0.1374, Steps: 234\n",
      "Episode: 397, Reward: 17.0, Epsilon: 0.1367, Steps: 425\n",
      "Episode: 398, Reward: 15.0, Epsilon: 0.1360, Steps: 305\n",
      "Target network updated at step 119000\n",
      "Episode: 399, Reward: 10.0, Epsilon: 0.1353, Steps: 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, Reward: 13.0, Epsilon: 0.1347, Steps: 284\n",
      "Model saved at episode 400\n",
      "Episode: 401, Reward: 19.0, Epsilon: 0.1340, Steps: 281\n",
      "Episode: 402, Reward: 19.0, Epsilon: 0.1333, Steps: 279\n",
      "Target network updated at step 120000\n",
      "Episode: 403, Reward: 10.0, Epsilon: 0.1326, Steps: 233\n",
      "Episode: 404, Reward: 16.0, Epsilon: 0.1320, Steps: 295\n",
      "Episode: 405, Reward: 16.0, Epsilon: 0.1313, Steps: 379\n",
      "Target network updated at step 121000\n",
      "Episode: 406, Reward: 10.0, Epsilon: 0.1307, Steps: 233\n",
      "Episode: 407, Reward: 13.0, Epsilon: 0.1300, Steps: 394\n",
      "Episode: 408, Reward: 22.0, Epsilon: 0.1294, Steps: 362\n",
      "Target network updated at step 122000\n",
      "Episode: 409, Reward: 13.0, Epsilon: 0.1287, Steps: 281\n",
      "Episode: 410, Reward: 14.0, Epsilon: 0.1281, Steps: 424\n",
      "Episode: 411, Reward: 19.0, Epsilon: 0.1274, Steps: 289\n",
      "Target network updated at step 123000\n",
      "Episode: 412, Reward: 23.0, Epsilon: 0.1268, Steps: 415\n",
      "Episode: 413, Reward: 10.0, Epsilon: 0.1262, Steps: 272\n",
      "Episode: 414, Reward: 15.0, Epsilon: 0.1255, Steps: 344\n",
      "Target network updated at step 124000\n",
      "Episode: 415, Reward: 18.0, Epsilon: 0.1249, Steps: 393\n",
      "Episode: 416, Reward: 17.0, Epsilon: 0.1243, Steps: 280\n",
      "Episode: 417, Reward: 20.0, Epsilon: 0.1237, Steps: 267\n",
      "Target network updated at step 125000\n",
      "Episode: 418, Reward: 14.0, Epsilon: 0.1230, Steps: 357\n",
      "Episode: 419, Reward: 17.0, Epsilon: 0.1224, Steps: 342\n",
      "Episode: 420, Reward: 17.0, Epsilon: 0.1218, Steps: 336\n",
      "Target network updated at step 126000\n",
      "Episode: 421, Reward: 18.0, Epsilon: 0.1212, Steps: 391\n",
      "Episode: 422, Reward: 12.0, Epsilon: 0.1206, Steps: 272\n",
      "Episode: 423, Reward: 14.0, Epsilon: 0.1200, Steps: 285\n",
      "Target network updated at step 127000\n",
      "Episode: 424, Reward: 19.0, Epsilon: 0.1194, Steps: 401\n",
      "Episode: 425, Reward: 17.0, Epsilon: 0.1188, Steps: 480\n",
      "Target network updated at step 128000\n",
      "Episode: 426, Reward: 16.0, Epsilon: 0.1182, Steps: 466\n",
      "Episode: 427, Reward: 21.0, Epsilon: 0.1176, Steps: 389\n",
      "Episode: 428, Reward: 15.0, Epsilon: 0.1170, Steps: 337\n",
      "Target network updated at step 129000\n",
      "Episode: 429, Reward: 20.0, Epsilon: 0.1164, Steps: 342\n",
      "Episode: 430, Reward: 10.0, Epsilon: 0.1159, Steps: 349\n",
      "Episode: 431, Reward: 17.0, Epsilon: 0.1153, Steps: 361\n",
      "Target network updated at step 130000\n",
      "Episode: 432, Reward: 19.0, Epsilon: 0.1147, Steps: 312\n",
      "Episode: 433, Reward: 17.0, Epsilon: 0.1141, Steps: 301\n",
      "Episode: 434, Reward: 14.0, Epsilon: 0.1136, Steps: 443\n",
      "Target network updated at step 131000\n",
      "Episode: 435, Reward: 12.0, Epsilon: 0.1130, Steps: 273\n",
      "Episode: 436, Reward: 17.0, Epsilon: 0.1124, Steps: 281\n",
      "Episode: 437, Reward: 18.0, Epsilon: 0.1119, Steps: 305\n",
      "Target network updated at step 132000\n",
      "Episode: 438, Reward: 13.0, Epsilon: 0.1113, Steps: 333\n",
      "Episode: 439, Reward: 19.0, Epsilon: 0.1107, Steps: 333\n",
      "Episode: 440, Reward: 9.0, Epsilon: 0.1102, Steps: 313\n",
      "Target network updated at step 133000\n",
      "Episode: 441, Reward: 21.0, Epsilon: 0.1096, Steps: 485\n",
      "Episode: 442, Reward: 20.0, Epsilon: 0.1091, Steps: 321\n",
      "Episode: 443, Reward: 19.0, Epsilon: 0.1085, Steps: 406\n",
      "Target network updated at step 134000\n",
      "Episode: 444, Reward: 16.0, Epsilon: 0.1080, Steps: 280\n",
      "Episode: 445, Reward: 20.0, Epsilon: 0.1075, Steps: 379\n",
      "Target network updated at step 135000\n",
      "Episode: 446, Reward: 17.0, Epsilon: 0.1069, Steps: 605\n",
      "Episode: 447, Reward: 18.0, Epsilon: 0.1064, Steps: 478\n",
      "Target network updated at step 136000\n",
      "Episode: 448, Reward: 18.0, Epsilon: 0.1059, Steps: 404\n",
      "Episode: 449, Reward: 13.0, Epsilon: 0.1053, Steps: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 450, Reward: 14.0, Epsilon: 0.1048, Steps: 281\n",
      "Model saved at episode 450\n",
      "Target network updated at step 137000\n",
      "Episode: 451, Reward: 17.0, Epsilon: 0.1043, Steps: 486\n",
      "Episode: 452, Reward: 18.0, Epsilon: 0.1038, Steps: 252\n",
      "Episode: 453, Reward: 20.0, Epsilon: 0.1032, Steps: 318\n",
      "Target network updated at step 138000\n",
      "Episode: 454, Reward: 21.0, Epsilon: 0.1027, Steps: 329\n",
      "Episode: 455, Reward: 19.0, Epsilon: 0.1022, Steps: 447\n",
      "Episode: 456, Reward: 14.0, Epsilon: 0.1017, Steps: 377\n",
      "Target network updated at step 139000\n",
      "Episode: 457, Reward: 20.0, Epsilon: 0.1012, Steps: 305\n",
      "Episode: 458, Reward: 16.0, Epsilon: 0.1007, Steps: 496\n",
      "Target network updated at step 140000\n",
      "Episode: 459, Reward: 16.0, Epsilon: 0.1002, Steps: 394\n",
      "Episode: 460, Reward: 18.0, Epsilon: 0.0997, Steps: 384\n",
      "Episode: 461, Reward: 16.0, Epsilon: 0.0992, Steps: 391\n",
      "Target network updated at step 141000\n",
      "Episode: 462, Reward: 15.0, Epsilon: 0.0987, Steps: 414\n",
      "Episode: 463, Reward: 16.0, Epsilon: 0.0982, Steps: 439\n",
      "Target network updated at step 142000\n",
      "Episode: 464, Reward: 15.0, Epsilon: 0.0977, Steps: 301\n",
      "Episode: 465, Reward: 16.0, Epsilon: 0.0972, Steps: 314\n",
      "Episode: 466, Reward: 17.0, Epsilon: 0.0967, Steps: 234\n",
      "Episode: 467, Reward: 9.0, Epsilon: 0.0962, Steps: 240\n",
      "Target network updated at step 143000\n",
      "Episode: 468, Reward: 16.0, Epsilon: 0.0958, Steps: 294\n",
      "Episode: 469, Reward: 12.0, Epsilon: 0.0953, Steps: 562\n",
      "Episode: 470, Reward: 16.0, Epsilon: 0.0948, Steps: 288\n",
      "Target network updated at step 144000\n",
      "Episode: 471, Reward: 17.0, Epsilon: 0.0943, Steps: 378\n",
      "Episode: 472, Reward: 19.0, Epsilon: 0.0939, Steps: 393\n",
      "Target network updated at step 145000\n",
      "Episode: 473, Reward: 16.0, Epsilon: 0.0934, Steps: 433\n",
      "Episode: 474, Reward: 16.0, Epsilon: 0.0929, Steps: 709\n",
      "Target network updated at step 146000\n",
      "Episode: 475, Reward: 18.0, Epsilon: 0.0925, Steps: 285\n",
      "Episode: 476, Reward: 24.0, Epsilon: 0.0920, Steps: 653\n",
      "Target network updated at step 147000\n",
      "Episode: 477, Reward: 13.0, Epsilon: 0.0915, Steps: 271\n",
      "Episode: 478, Reward: 18.0, Epsilon: 0.0911, Steps: 554\n",
      "Target network updated at step 148000\n",
      "Episode: 479, Reward: 18.0, Epsilon: 0.0906, Steps: 478\n",
      "Episode: 480, Reward: 19.0, Epsilon: 0.0902, Steps: 327\n",
      "Target network updated at step 149000\n",
      "Episode: 481, Reward: 15.0, Epsilon: 0.0897, Steps: 632\n",
      "Episode: 482, Reward: 17.0, Epsilon: 0.0893, Steps: 616\n",
      "Target network updated at step 150000\n",
      "Episode: 483, Reward: 17.0, Epsilon: 0.0888, Steps: 565\n",
      "Episode: 484, Reward: 17.0, Epsilon: 0.0884, Steps: 293\n",
      "Episode: 485, Reward: 9.0, Epsilon: 0.0879, Steps: 332\n",
      "Target network updated at step 151000\n",
      "Episode: 486, Reward: 15.0, Epsilon: 0.0875, Steps: 459\n",
      "Episode: 487, Reward: 17.0, Epsilon: 0.0871, Steps: 537\n",
      "Target network updated at step 152000\n",
      "Episode: 488, Reward: 12.0, Epsilon: 0.0866, Steps: 370\n",
      "Episode: 489, Reward: 18.0, Epsilon: 0.0862, Steps: 388\n",
      "Episode: 490, Reward: 18.0, Epsilon: 0.0858, Steps: 282\n",
      "Target network updated at step 153000\n",
      "Episode: 491, Reward: 17.0, Epsilon: 0.0853, Steps: 283\n",
      "Episode: 492, Reward: 17.0, Epsilon: 0.0849, Steps: 306\n",
      "Episode: 493, Reward: 14.0, Epsilon: 0.0845, Steps: 328\n",
      "Target network updated at step 154000\n",
      "Episode: 494, Reward: 15.0, Epsilon: 0.0841, Steps: 321\n",
      "Episode: 495, Reward: 18.0, Epsilon: 0.0836, Steps: 389\n",
      "Episode: 496, Reward: 9.0, Epsilon: 0.0832, Steps: 228\n",
      "Target network updated at step 155000\n",
      "Episode: 497, Reward: 17.0, Epsilon: 0.0828, Steps: 448\n",
      "Episode: 498, Reward: 12.0, Epsilon: 0.0824, Steps: 274\n",
      "Episode: 499, Reward: 18.0, Epsilon: 0.0820, Steps: 286\n",
      "Target network updated at step 156000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, Reward: 18.0, Epsilon: 0.0816, Steps: 305\n",
      "Model saved at episode 500\n",
      "Episode: 501, Reward: 16.0, Epsilon: 0.0812, Steps: 369\n",
      "Episode: 502, Reward: 20.0, Epsilon: 0.0808, Steps: 302\n",
      "Target network updated at step 157000\n",
      "Episode: 503, Reward: 15.0, Epsilon: 0.0804, Steps: 735\n",
      "Episode: 504, Reward: 18.0, Epsilon: 0.0800, Steps: 276\n",
      "Target network updated at step 158000\n",
      "Episode: 505, Reward: 17.0, Epsilon: 0.0796, Steps: 713\n",
      "Target network updated at step 159000\n",
      "Episode: 506, Reward: 23.0, Epsilon: 0.0792, Steps: 793\n",
      "Episode: 507, Reward: 13.0, Epsilon: 0.0788, Steps: 395\n",
      "Episode: 508, Reward: 15.0, Epsilon: 0.0784, Steps: 318\n",
      "Target network updated at step 160000\n",
      "Episode: 509, Reward: 12.0, Epsilon: 0.0780, Steps: 312\n",
      "Episode: 510, Reward: 18.0, Epsilon: 0.0776, Steps: 320\n",
      "Episode: 511, Reward: 8.0, Epsilon: 0.0772, Steps: 306\n",
      "Target network updated at step 161000\n",
      "Episode: 512, Reward: 22.0, Epsilon: 0.0768, Steps: 705\n",
      "Target network updated at step 162000\n",
      "Episode: 513, Reward: 18.0, Epsilon: 0.0764, Steps: 717\n",
      "Episode: 514, Reward: 11.0, Epsilon: 0.0760, Steps: 363\n",
      "Episode: 515, Reward: 12.0, Epsilon: 0.0757, Steps: 271\n",
      "Target network updated at step 163000\n",
      "Episode: 516, Reward: 14.0, Epsilon: 0.0753, Steps: 342\n",
      "Target network updated at step 164000\n",
      "Episode: 517, Reward: 19.0, Epsilon: 0.0749, Steps: 701\n",
      "Episode: 518, Reward: 11.0, Epsilon: 0.0745, Steps: 236\n",
      "Episode: 519, Reward: 18.0, Epsilon: 0.0742, Steps: 410\n",
      "Target network updated at step 165000\n",
      "Episode: 520, Reward: 19.0, Epsilon: 0.0738, Steps: 554\n",
      "Episode: 521, Reward: 21.0, Epsilon: 0.0734, Steps: 389\n",
      "Target network updated at step 166000\n",
      "Episode: 522, Reward: 13.0, Epsilon: 0.0731, Steps: 422\n",
      "Episode: 523, Reward: 19.0, Epsilon: 0.0727, Steps: 244\n",
      "Episode: 524, Reward: 18.0, Epsilon: 0.0723, Steps: 342\n",
      "Episode: 525, Reward: 13.0, Epsilon: 0.0720, Steps: 321\n",
      "Target network updated at step 167000\n",
      "Episode: 526, Reward: 13.0, Epsilon: 0.0716, Steps: 735\n",
      "Episode: 527, Reward: 14.0, Epsilon: 0.0712, Steps: 319\n",
      "Target network updated at step 168000\n",
      "Episode: 528, Reward: 10.0, Epsilon: 0.0709, Steps: 226\n",
      "Episode: 529, Reward: 16.0, Epsilon: 0.0705, Steps: 332\n",
      "Episode: 530, Reward: 11.0, Epsilon: 0.0702, Steps: 235\n",
      "Target network updated at step 169000\n",
      "Episode: 531, Reward: 10.0, Epsilon: 0.0698, Steps: 236\n",
      "Episode: 532, Reward: 16.0, Epsilon: 0.0695, Steps: 428\n",
      "Episode: 533, Reward: 19.0, Epsilon: 0.0691, Steps: 298\n",
      "Target network updated at step 170000\n",
      "Episode: 534, Reward: 15.0, Epsilon: 0.0688, Steps: 643\n",
      "Episode: 535, Reward: 19.0, Epsilon: 0.0684, Steps: 308\n",
      "Episode: 536, Reward: 14.0, Epsilon: 0.0681, Steps: 250\n",
      "Target network updated at step 171000\n",
      "Episode: 537, Reward: 21.0, Epsilon: 0.0678, Steps: 336\n",
      "Episode: 538, Reward: 17.0, Epsilon: 0.0674, Steps: 506\n",
      "Target network updated at step 172000\n",
      "Episode: 539, Reward: 13.0, Epsilon: 0.0671, Steps: 703\n",
      "Episode: 540, Reward: 16.0, Epsilon: 0.0668, Steps: 251\n",
      "Target network updated at step 173000\n",
      "Episode: 541, Reward: 20.0, Epsilon: 0.0664, Steps: 349\n",
      "Episode: 542, Reward: 16.0, Epsilon: 0.0661, Steps: 497\n",
      "Episode: 543, Reward: 12.0, Epsilon: 0.0658, Steps: 269\n",
      "Target network updated at step 174000\n",
      "Episode: 544, Reward: 15.0, Epsilon: 0.0654, Steps: 703\n",
      "Episode: 545, Reward: 14.0, Epsilon: 0.0651, Steps: 253\n",
      "Target network updated at step 175000\n",
      "Episode: 546, Reward: 17.0, Epsilon: 0.0648, Steps: 336\n",
      "Episode: 547, Reward: 13.0, Epsilon: 0.0645, Steps: 420\n",
      "Episode: 548, Reward: 18.0, Epsilon: 0.0641, Steps: 365\n",
      "Target network updated at step 176000\n",
      "Episode: 549, Reward: 18.0, Epsilon: 0.0638, Steps: 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 550, Reward: 13.0, Epsilon: 0.0635, Steps: 291\n",
      "Model saved at episode 550\n",
      "Episode: 551, Reward: 17.0, Epsilon: 0.0632, Steps: 275\n",
      "Target network updated at step 177000\n",
      "Episode: 552, Reward: 16.0, Epsilon: 0.0629, Steps: 342\n",
      "Episode: 553, Reward: 9.0, Epsilon: 0.0625, Steps: 409\n",
      "Episode: 554, Reward: 16.0, Epsilon: 0.0622, Steps: 258\n",
      "Target network updated at step 178000\n",
      "Episode: 555, Reward: 26.0, Epsilon: 0.0619, Steps: 495\n",
      "Episode: 556, Reward: 16.0, Epsilon: 0.0616, Steps: 329\n",
      "Episode: 557, Reward: 13.0, Epsilon: 0.0613, Steps: 283\n",
      "Target network updated at step 179000\n",
      "Episode: 558, Reward: 12.0, Epsilon: 0.0610, Steps: 393\n",
      "Episode: 559, Reward: 10.0, Epsilon: 0.0607, Steps: 230\n",
      "Episode: 560, Reward: 14.0, Epsilon: 0.0604, Steps: 458\n",
      "Target network updated at step 180000\n",
      "Episode: 561, Reward: 18.0, Epsilon: 0.0601, Steps: 570\n",
      "Episode: 562, Reward: 20.0, Epsilon: 0.0598, Steps: 440\n",
      "Target network updated at step 181000\n",
      "Episode: 563, Reward: 14.0, Epsilon: 0.0595, Steps: 284\n",
      "Episode: 564, Reward: 10.0, Epsilon: 0.0592, Steps: 219\n",
      "Target network updated at step 182000\n",
      "Episode: 565, Reward: 15.0, Epsilon: 0.0589, Steps: 616\n",
      "Episode: 566, Reward: 21.0, Epsilon: 0.0586, Steps: 360\n",
      "Episode: 567, Reward: 17.0, Epsilon: 0.0583, Steps: 462\n",
      "Target network updated at step 183000\n",
      "Episode: 568, Reward: 16.0, Epsilon: 0.0580, Steps: 315\n",
      "Episode: 569, Reward: 19.0, Epsilon: 0.0577, Steps: 290\n",
      "Target network updated at step 184000\n",
      "Episode: 570, Reward: 22.0, Epsilon: 0.0574, Steps: 847\n",
      "Episode: 571, Reward: 19.0, Epsilon: 0.0571, Steps: 279\n",
      "Target network updated at step 185000\n",
      "Episode: 572, Reward: 16.0, Epsilon: 0.0569, Steps: 601\n",
      "Episode: 573, Reward: 23.0, Epsilon: 0.0566, Steps: 556\n",
      "Target network updated at step 186000\n",
      "Episode: 574, Reward: 16.0, Epsilon: 0.0563, Steps: 312\n",
      "Episode: 575, Reward: 18.0, Epsilon: 0.0560, Steps: 419\n",
      "Episode: 576, Reward: 17.0, Epsilon: 0.0557, Steps: 281\n",
      "Target network updated at step 187000\n",
      "Episode: 577, Reward: 20.0, Epsilon: 0.0555, Steps: 307\n",
      "Episode: 578, Reward: 16.0, Epsilon: 0.0552, Steps: 703\n",
      "Target network updated at step 188000\n",
      "Episode: 579, Reward: 17.0, Epsilon: 0.0549, Steps: 653\n",
      "Target network updated at step 189000\n",
      "Episode: 580, Reward: 22.0, Epsilon: 0.0546, Steps: 585\n",
      "Episode: 581, Reward: 18.0, Epsilon: 0.0544, Steps: 735\n",
      "Target network updated at step 190000\n",
      "Episode: 582, Reward: 16.0, Epsilon: 0.0541, Steps: 297\n",
      "Episode: 583, Reward: 18.0, Epsilon: 0.0538, Steps: 549\n",
      "Episode: 584, Reward: 17.0, Epsilon: 0.0535, Steps: 297\n",
      "Target network updated at step 191000\n",
      "Episode: 585, Reward: 21.0, Epsilon: 0.0533, Steps: 281\n",
      "Episode: 586, Reward: 27.0, Epsilon: 0.0530, Steps: 409\n",
      "Episode: 587, Reward: 13.0, Epsilon: 0.0527, Steps: 249\n",
      "Target network updated at step 192000\n",
      "Episode: 588, Reward: 11.0, Epsilon: 0.0525, Steps: 222\n",
      "Episode: 589, Reward: 11.0, Epsilon: 0.0522, Steps: 312\n",
      "Target network updated at step 193000\n",
      "Episode: 590, Reward: 21.0, Epsilon: 0.0520, Steps: 719\n",
      "Episode: 591, Reward: 15.0, Epsilon: 0.0517, Steps: 452\n",
      "Episode: 592, Reward: 15.0, Epsilon: 0.0514, Steps: 329\n",
      "Target network updated at step 194000\n",
      "Episode: 593, Reward: 19.0, Epsilon: 0.0512, Steps: 691\n",
      "Target network updated at step 195000\n",
      "Episode: 594, Reward: 17.0, Epsilon: 0.0509, Steps: 465\n",
      "Episode: 595, Reward: 19.0, Epsilon: 0.0507, Steps: 490\n",
      "Target network updated at step 196000\n",
      "Episode: 596, Reward: 19.0, Epsilon: 0.0504, Steps: 712\n",
      "Episode: 597, Reward: 21.0, Epsilon: 0.0502, Steps: 349\n",
      "Target network updated at step 197000\n",
      "Episode: 598, Reward: 26.0, Epsilon: 0.0499, Steps: 813\n",
      "Episode: 599, Reward: 19.0, Epsilon: 0.0497, Steps: 387\n",
      "Target network updated at step 198000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 600, Reward: 14.0, Epsilon: 0.0494, Steps: 337\n",
      "Model saved at episode 600\n",
      "Episode: 601, Reward: 13.0, Epsilon: 0.0492, Steps: 250\n",
      "Episode: 602, Reward: 12.0, Epsilon: 0.0489, Steps: 232\n",
      "Episode: 603, Reward: 13.0, Epsilon: 0.0487, Steps: 336\n",
      "Target network updated at step 199000\n",
      "Episode: 604, Reward: 19.0, Epsilon: 0.0484, Steps: 301\n",
      "Episode: 605, Reward: 15.0, Epsilon: 0.0482, Steps: 292\n",
      "Episode: 606, Reward: 16.0, Epsilon: 0.0479, Steps: 265\n",
      "Target network updated at step 200000\n",
      "Episode: 607, Reward: 8.0, Epsilon: 0.0477, Steps: 326\n",
      "Episode: 608, Reward: 17.0, Epsilon: 0.0475, Steps: 297\n",
      "Episode: 609, Reward: 16.0, Epsilon: 0.0472, Steps: 319\n",
      "Target network updated at step 201000\n",
      "Episode: 610, Reward: 22.0, Epsilon: 0.0470, Steps: 363\n",
      "Episode: 611, Reward: 18.0, Epsilon: 0.0468, Steps: 293\n",
      "Episode: 612, Reward: 15.0, Epsilon: 0.0465, Steps: 281\n",
      "Target network updated at step 202000\n",
      "Episode: 613, Reward: 22.0, Epsilon: 0.0463, Steps: 359\n",
      "Episode: 614, Reward: 15.0, Epsilon: 0.0461, Steps: 329\n",
      "Episode: 615, Reward: 11.0, Epsilon: 0.0458, Steps: 367\n",
      "Target network updated at step 203000\n",
      "Episode: 616, Reward: 15.0, Epsilon: 0.0456, Steps: 687\n",
      "Episode: 617, Reward: 23.0, Epsilon: 0.0454, Steps: 367\n",
      "Target network updated at step 204000\n",
      "Episode: 618, Reward: 20.0, Epsilon: 0.0452, Steps: 354\n",
      "Episode: 619, Reward: 20.0, Epsilon: 0.0449, Steps: 392\n",
      "Episode: 620, Reward: 17.0, Epsilon: 0.0447, Steps: 274\n",
      "Target network updated at step 205000\n",
      "Episode: 621, Reward: 9.0, Epsilon: 0.0445, Steps: 274\n",
      "Episode: 622, Reward: 18.0, Epsilon: 0.0443, Steps: 273\n",
      "Episode: 623, Reward: 14.0, Epsilon: 0.0440, Steps: 303\n",
      "Episode: 624, Reward: 19.0, Epsilon: 0.0438, Steps: 296\n",
      "Target network updated at step 206000\n",
      "Episode: 625, Reward: 8.0, Epsilon: 0.0436, Steps: 265\n",
      "Episode: 626, Reward: 15.0, Epsilon: 0.0434, Steps: 367\n",
      "Episode: 627, Reward: 15.0, Epsilon: 0.0432, Steps: 303\n",
      "Target network updated at step 207000\n",
      "Episode: 628, Reward: 12.0, Epsilon: 0.0429, Steps: 319\n",
      "Episode: 629, Reward: 17.0, Epsilon: 0.0427, Steps: 403\n",
      "Target network updated at step 208000\n",
      "Episode: 630, Reward: 18.0, Epsilon: 0.0425, Steps: 570\n",
      "Episode: 631, Reward: 15.0, Epsilon: 0.0423, Steps: 226\n",
      "Target network updated at step 209000\n",
      "Episode: 632, Reward: 14.0, Epsilon: 0.0421, Steps: 671\n",
      "Episode: 633, Reward: 12.0, Epsilon: 0.0419, Steps: 703\n",
      "Target network updated at step 210000\n",
      "Episode: 634, Reward: 19.0, Epsilon: 0.0417, Steps: 333\n",
      "Episode: 635, Reward: 17.0, Epsilon: 0.0415, Steps: 285\n",
      "Episode: 636, Reward: 14.0, Epsilon: 0.0413, Steps: 292\n",
      "Target network updated at step 211000\n",
      "Episode: 637, Reward: 18.0, Epsilon: 0.0410, Steps: 280\n",
      "Episode: 638, Reward: 14.0, Epsilon: 0.0408, Steps: 235\n",
      "Episode: 639, Reward: 15.0, Epsilon: 0.0406, Steps: 318\n",
      "Target network updated at step 212000\n",
      "Episode: 640, Reward: 15.0, Epsilon: 0.0404, Steps: 719\n",
      "Episode: 641, Reward: 18.0, Epsilon: 0.0402, Steps: 342\n",
      "Target network updated at step 213000\n",
      "Episode: 642, Reward: 15.0, Epsilon: 0.0400, Steps: 431\n",
      "Episode: 643, Reward: 14.0, Epsilon: 0.0398, Steps: 287\n",
      "Episode: 644, Reward: 14.0, Epsilon: 0.0396, Steps: 393\n",
      "Episode: 645, Reward: 9.0, Epsilon: 0.0394, Steps: 256\n",
      "Target network updated at step 214000\n",
      "Episode: 646, Reward: 16.0, Epsilon: 0.0392, Steps: 379\n",
      "Episode: 647, Reward: 16.0, Epsilon: 0.0390, Steps: 387\n",
      "Episode: 648, Reward: 9.0, Epsilon: 0.0388, Steps: 219\n",
      "Target network updated at step 215000\n",
      "Episode: 649, Reward: 24.0, Epsilon: 0.0387, Steps: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 650, Reward: 15.0, Epsilon: 0.0385, Steps: 317\n",
      "Model saved at episode 650\n",
      "Target network updated at step 216000\n",
      "Episode: 651, Reward: 14.0, Epsilon: 0.0383, Steps: 290\n",
      "Episode: 652, Reward: 14.0, Epsilon: 0.0381, Steps: 385\n",
      "Episode: 653, Reward: 14.0, Epsilon: 0.0379, Steps: 386\n",
      "Target network updated at step 217000\n",
      "Episode: 654, Reward: 14.0, Epsilon: 0.0377, Steps: 281\n",
      "Episode: 655, Reward: 15.0, Epsilon: 0.0375, Steps: 365\n",
      "Episode: 656, Reward: 17.0, Epsilon: 0.0373, Steps: 383\n",
      "Target network updated at step 218000\n",
      "Episode: 657, Reward: 15.0, Epsilon: 0.0371, Steps: 703\n",
      "Episode: 658, Reward: 10.0, Epsilon: 0.0369, Steps: 237\n",
      "Target network updated at step 219000\n",
      "Episode: 659, Reward: 12.0, Epsilon: 0.0368, Steps: 516\n",
      "Episode: 660, Reward: 9.0, Epsilon: 0.0366, Steps: 307\n",
      "Target network updated at step 220000\n",
      "Episode: 661, Reward: 15.0, Epsilon: 0.0364, Steps: 719\n",
      "Episode: 662, Reward: 9.0, Epsilon: 0.0362, Steps: 221\n",
      "Episode: 663, Reward: 15.0, Epsilon: 0.0360, Steps: 283\n",
      "Target network updated at step 221000\n",
      "Episode: 664, Reward: 13.0, Epsilon: 0.0359, Steps: 342\n",
      "Episode: 665, Reward: 14.0, Epsilon: 0.0357, Steps: 286\n",
      "Episode: 666, Reward: 16.0, Epsilon: 0.0355, Steps: 481\n",
      "Target network updated at step 222000\n",
      "Episode: 667, Reward: 13.0, Epsilon: 0.0353, Steps: 425\n",
      "Episode: 668, Reward: 7.0, Epsilon: 0.0351, Steps: 290\n",
      "Target network updated at step 223000\n",
      "Episode: 669, Reward: 13.0, Epsilon: 0.0350, Steps: 633\n",
      "Episode: 670, Reward: 16.0, Epsilon: 0.0348, Steps: 703\n",
      "Target network updated at step 224000\n",
      "Episode: 671, Reward: 16.0, Epsilon: 0.0346, Steps: 298\n",
      "Episode: 672, Reward: 15.0, Epsilon: 0.0344, Steps: 305\n",
      "Episode: 673, Reward: 17.0, Epsilon: 0.0343, Steps: 401\n",
      "Target network updated at step 225000\n",
      "Episode: 674, Reward: 14.0, Epsilon: 0.0341, Steps: 552\n",
      "Episode: 675, Reward: 19.0, Epsilon: 0.0339, Steps: 359\n",
      "Target network updated at step 226000\n",
      "Episode: 676, Reward: 14.0, Epsilon: 0.0338, Steps: 463\n",
      "Episode: 677, Reward: 10.0, Epsilon: 0.0336, Steps: 232\n",
      "Target network updated at step 227000\n",
      "Episode: 678, Reward: 15.0, Epsilon: 0.0334, Steps: 703\n",
      "Episode: 679, Reward: 12.0, Epsilon: 0.0333, Steps: 233\n",
      "Episode: 680, Reward: 9.0, Epsilon: 0.0331, Steps: 425\n",
      "Target network updated at step 228000\n",
      "Episode: 681, Reward: 11.0, Epsilon: 0.0329, Steps: 473\n",
      "Episode: 682, Reward: 15.0, Epsilon: 0.0328, Steps: 330\n",
      "Target network updated at step 229000\n",
      "Episode: 683, Reward: 9.0, Epsilon: 0.0326, Steps: 299\n",
      "Episode: 684, Reward: 15.0, Epsilon: 0.0324, Steps: 719\n",
      "Target network updated at step 230000\n",
      "Episode: 685, Reward: 17.0, Epsilon: 0.0323, Steps: 367\n",
      "Episode: 686, Reward: 14.0, Epsilon: 0.0321, Steps: 336\n",
      "Episode: 687, Reward: 16.0, Epsilon: 0.0319, Steps: 394\n",
      "Target network updated at step 231000\n",
      "Episode: 688, Reward: 14.0, Epsilon: 0.0318, Steps: 633\n",
      "Target network updated at step 232000\n",
      "Episode: 689, Reward: 12.0, Epsilon: 0.0316, Steps: 641\n",
      "Episode: 690, Reward: 24.0, Epsilon: 0.0315, Steps: 617\n",
      "Target network updated at step 233000\n",
      "Episode: 691, Reward: 14.0, Epsilon: 0.0313, Steps: 341\n",
      "Episode: 692, Reward: 19.0, Epsilon: 0.0312, Steps: 385\n",
      "Target network updated at step 234000\n",
      "Episode: 693, Reward: 18.0, Epsilon: 0.0310, Steps: 703\n",
      "Episode: 694, Reward: 15.0, Epsilon: 0.0308, Steps: 687\n",
      "Target network updated at step 235000\n",
      "Episode: 695, Reward: 15.0, Epsilon: 0.0307, Steps: 590\n",
      "Target network updated at step 236000\n",
      "Episode: 696, Reward: 21.0, Epsilon: 0.0305, Steps: 735\n",
      "Episode: 697, Reward: 20.0, Epsilon: 0.0304, Steps: 525\n",
      "Target network updated at step 237000\n",
      "Episode: 698, Reward: 18.0, Epsilon: 0.0302, Steps: 525\n",
      "Episode: 699, Reward: 16.0, Epsilon: 0.0301, Steps: 712\n",
      "Target network updated at step 238000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 700, Reward: 15.0, Epsilon: 0.0299, Steps: 767\n",
      "Model saved at episode 700\n",
      "Target network updated at step 239000\n",
      "Episode: 701, Reward: 15.0, Epsilon: 0.0298, Steps: 550\n",
      "Episode: 702, Reward: 19.0, Epsilon: 0.0296, Steps: 388\n",
      "Episode: 703, Reward: 17.0, Epsilon: 0.0295, Steps: 237\n",
      "Target network updated at step 240000\n",
      "Episode: 704, Reward: 12.0, Epsilon: 0.0293, Steps: 392\n",
      "Episode: 705, Reward: 17.0, Epsilon: 0.0292, Steps: 297\n",
      "Target network updated at step 241000\n",
      "Episode: 706, Reward: 16.0, Epsilon: 0.0290, Steps: 547\n",
      "Episode: 707, Reward: 15.0, Epsilon: 0.0289, Steps: 386\n",
      "Episode: 708, Reward: 18.0, Epsilon: 0.0288, Steps: 321\n",
      "Target network updated at step 242000\n",
      "Episode: 709, Reward: 12.0, Epsilon: 0.0286, Steps: 719\n",
      "Target network updated at step 243000\n",
      "Episode: 710, Reward: 22.0, Epsilon: 0.0285, Steps: 815\n",
      "Episode: 711, Reward: 14.0, Epsilon: 0.0283, Steps: 289\n",
      "Target network updated at step 244000\n",
      "Episode: 712, Reward: 14.0, Epsilon: 0.0282, Steps: 555\n",
      "Episode: 713, Reward: 15.0, Epsilon: 0.0280, Steps: 425\n",
      "Target network updated at step 245000\n",
      "Episode: 714, Reward: 18.0, Epsilon: 0.0279, Steps: 718\n",
      "Episode: 715, Reward: 21.0, Epsilon: 0.0278, Steps: 375\n",
      "Target network updated at step 246000\n",
      "Episode: 716, Reward: 18.0, Epsilon: 0.0276, Steps: 320\n",
      "Episode: 717, Reward: 15.0, Epsilon: 0.0275, Steps: 687\n",
      "Target network updated at step 247000\n",
      "Episode: 718, Reward: 24.0, Epsilon: 0.0274, Steps: 712\n",
      "Target network updated at step 248000\n",
      "Episode: 719, Reward: 17.0, Epsilon: 0.0272, Steps: 753\n",
      "Episode: 720, Reward: 12.0, Epsilon: 0.0271, Steps: 675\n",
      "Target network updated at step 249000\n",
      "Episode: 721, Reward: 11.0, Epsilon: 0.0269, Steps: 380\n",
      "Episode: 722, Reward: 13.0, Epsilon: 0.0268, Steps: 289\n",
      "Episode: 723, Reward: 15.0, Epsilon: 0.0267, Steps: 289\n",
      "Target network updated at step 250000\n",
      "Episode: 724, Reward: 16.0, Epsilon: 0.0265, Steps: 767\n",
      "Episode: 725, Reward: 13.0, Epsilon: 0.0264, Steps: 285\n",
      "Target network updated at step 251000\n",
      "Episode: 726, Reward: 17.0, Epsilon: 0.0263, Steps: 351\n",
      "Episode: 727, Reward: 12.0, Epsilon: 0.0261, Steps: 281\n",
      "Episode: 728, Reward: 14.0, Epsilon: 0.0260, Steps: 448\n",
      "Target network updated at step 252000\n",
      "Episode: 729, Reward: 17.0, Epsilon: 0.0259, Steps: 512\n",
      "Episode: 730, Reward: 15.0, Epsilon: 0.0258, Steps: 235\n",
      "Target network updated at step 253000\n",
      "Episode: 731, Reward: 16.0, Epsilon: 0.0256, Steps: 369\n",
      "Episode: 732, Reward: 13.0, Epsilon: 0.0255, Steps: 703\n",
      "Target network updated at step 254000\n",
      "Episode: 733, Reward: 13.0, Epsilon: 0.0254, Steps: 301\n",
      "Episode: 734, Reward: 20.0, Epsilon: 0.0252, Steps: 496\n",
      "Target network updated at step 255000\n",
      "Episode: 735, Reward: 17.0, Epsilon: 0.0251, Steps: 719\n",
      "Episode: 736, Reward: 17.0, Epsilon: 0.0250, Steps: 529\n",
      "Target network updated at step 256000\n",
      "Episode: 737, Reward: 12.0, Epsilon: 0.0249, Steps: 305\n",
      "Episode: 738, Reward: 20.0, Epsilon: 0.0247, Steps: 597\n",
      "Target network updated at step 257000\n",
      "Episode: 739, Reward: 15.0, Epsilon: 0.0246, Steps: 703\n",
      "Episode: 740, Reward: 14.0, Epsilon: 0.0245, Steps: 349\n",
      "Target network updated at step 258000\n",
      "Episode: 741, Reward: 13.0, Epsilon: 0.0244, Steps: 719\n",
      "Target network updated at step 259000\n",
      "Episode: 742, Reward: 18.0, Epsilon: 0.0243, Steps: 719\n",
      "Episode: 743, Reward: 12.0, Epsilon: 0.0241, Steps: 449\n",
      "Target network updated at step 260000\n",
      "Episode: 744, Reward: 13.0, Epsilon: 0.0240, Steps: 372\n",
      "Episode: 745, Reward: 11.0, Epsilon: 0.0239, Steps: 405\n",
      "Episode: 746, Reward: 22.0, Epsilon: 0.0238, Steps: 445\n",
      "Target network updated at step 261000\n",
      "Episode: 747, Reward: 20.0, Epsilon: 0.0237, Steps: 537\n",
      "Episode: 748, Reward: 20.0, Epsilon: 0.0235, Steps: 531\n",
      "Target network updated at step 262000\n",
      "Episode: 749, Reward: 26.0, Epsilon: 0.0234, Steps: 537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 750, Reward: 20.0, Epsilon: 0.0233, Steps: 333\n",
      "Model saved at episode 750\n",
      "Target network updated at step 263000\n",
      "Episode: 751, Reward: 19.0, Epsilon: 0.0232, Steps: 363\n",
      "Episode: 752, Reward: 22.0, Epsilon: 0.0231, Steps: 479\n",
      "Target network updated at step 264000\n",
      "Episode: 753, Reward: 23.0, Epsilon: 0.0229, Steps: 535\n",
      "Episode: 754, Reward: 27.0, Epsilon: 0.0228, Steps: 441\n",
      "Target network updated at step 265000\n",
      "Episode: 755, Reward: 18.0, Epsilon: 0.0227, Steps: 703\n",
      "Episode: 756, Reward: 16.0, Epsilon: 0.0226, Steps: 550\n",
      "Target network updated at step 266000\n",
      "Episode: 757, Reward: 21.0, Epsilon: 0.0225, Steps: 545\n",
      "Target network updated at step 267000\n",
      "Episode: 758, Reward: 20.0, Epsilon: 0.0224, Steps: 719\n",
      "Episode: 759, Reward: 18.0, Epsilon: 0.0223, Steps: 395\n",
      "Episode: 760, Reward: 18.0, Epsilon: 0.0222, Steps: 237\n",
      "Target network updated at step 268000\n",
      "Episode: 761, Reward: 19.0, Epsilon: 0.0220, Steps: 735\n",
      "Target network updated at step 269000\n",
      "Episode: 762, Reward: 14.0, Epsilon: 0.0219, Steps: 687\n",
      "Episode: 763, Reward: 19.0, Epsilon: 0.0218, Steps: 250\n",
      "Episode: 764, Reward: 19.0, Epsilon: 0.0217, Steps: 355\n",
      "Target network updated at step 270000\n",
      "Episode: 765, Reward: 19.0, Epsilon: 0.0216, Steps: 276\n",
      "Episode: 766, Reward: 12.0, Epsilon: 0.0215, Steps: 228\n",
      "Episode: 767, Reward: 22.0, Epsilon: 0.0214, Steps: 399\n",
      "Target network updated at step 271000\n",
      "Episode: 768, Reward: 21.0, Epsilon: 0.0213, Steps: 395\n",
      "Episode: 769, Reward: 12.0, Epsilon: 0.0212, Steps: 332\n",
      "Episode: 770, Reward: 19.0, Epsilon: 0.0211, Steps: 250\n",
      "Episode: 771, Reward: 19.0, Epsilon: 0.0210, Steps: 251\n",
      "Target network updated at step 272000\n",
      "Episode: 772, Reward: 22.0, Epsilon: 0.0209, Steps: 351\n",
      "Episode: 773, Reward: 20.0, Epsilon: 0.0208, Steps: 449\n",
      "Target network updated at step 273000\n",
      "Episode: 774, Reward: 20.0, Epsilon: 0.0207, Steps: 399\n",
      "Episode: 775, Reward: 21.0, Epsilon: 0.0206, Steps: 574\n",
      "Target network updated at step 274000\n",
      "Episode: 776, Reward: 19.0, Epsilon: 0.0205, Steps: 324\n",
      "Episode: 777, Reward: 20.0, Epsilon: 0.0203, Steps: 322\n",
      "Episode: 778, Reward: 15.0, Epsilon: 0.0202, Steps: 393\n",
      "Target network updated at step 275000\n",
      "Episode: 779, Reward: 16.0, Epsilon: 0.0201, Steps: 344\n",
      "Episode: 780, Reward: 22.0, Epsilon: 0.0200, Steps: 374\n",
      "Episode: 781, Reward: 17.0, Epsilon: 0.0199, Steps: 333\n",
      "Target network updated at step 276000\n",
      "Episode: 782, Reward: 18.0, Epsilon: 0.0198, Steps: 396\n",
      "Episode: 783, Reward: 25.0, Epsilon: 0.0197, Steps: 772\n",
      "Target network updated at step 277000\n",
      "Episode: 784, Reward: 9.0, Epsilon: 0.0196, Steps: 266\n",
      "Target network updated at step 278000\n",
      "Episode: 785, Reward: 19.0, Epsilon: 0.0195, Steps: 799\n",
      "Episode: 786, Reward: 19.0, Epsilon: 0.0195, Steps: 656\n",
      "Target network updated at step 279000\n",
      "Episode: 787, Reward: 20.0, Epsilon: 0.0194, Steps: 765\n",
      "Target network updated at step 280000\n",
      "Episode: 788, Reward: 17.0, Epsilon: 0.0193, Steps: 687\n",
      "Episode: 789, Reward: 19.0, Epsilon: 0.0192, Steps: 513\n",
      "Episode: 790, Reward: 22.0, Epsilon: 0.0191, Steps: 343\n",
      "Target network updated at step 281000\n",
      "Episode: 791, Reward: 21.0, Epsilon: 0.0190, Steps: 370\n",
      "Target network updated at step 282000\n",
      "Episode: 792, Reward: 15.0, Epsilon: 0.0189, Steps: 767\n",
      "Episode: 793, Reward: 14.0, Epsilon: 0.0188, Steps: 419\n",
      "Target network updated at step 283000\n",
      "Episode: 794, Reward: 12.0, Epsilon: 0.0187, Steps: 751\n",
      "Episode: 795, Reward: 25.0, Epsilon: 0.0186, Steps: 541\n",
      "Target network updated at step 284000\n",
      "Episode: 796, Reward: 21.0, Epsilon: 0.0185, Steps: 553\n",
      "Episode: 797, Reward: 18.0, Epsilon: 0.0184, Steps: 269\n",
      "Target network updated at step 285000\n",
      "Episode: 798, Reward: 23.0, Epsilon: 0.0183, Steps: 413\n",
      "Episode: 799, Reward: 18.0, Epsilon: 0.0182, Steps: 540\n",
      "Target network updated at step 286000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 800, Reward: 22.0, Epsilon: 0.0181, Steps: 381\n",
      "Model saved at episode 800\n",
      "Episode: 801, Reward: 28.0, Epsilon: 0.0180, Steps: 750\n",
      "Target network updated at step 287000\n",
      "Episode: 802, Reward: 23.0, Epsilon: 0.0180, Steps: 703\n",
      "Target network updated at step 288000\n",
      "Episode: 803, Reward: 20.0, Epsilon: 0.0179, Steps: 719\n",
      "Episode: 804, Reward: 22.0, Epsilon: 0.0178, Steps: 343\n",
      "Episode: 805, Reward: 14.0, Epsilon: 0.0177, Steps: 317\n",
      "Target network updated at step 289000\n",
      "Episode: 806, Reward: 17.0, Epsilon: 0.0176, Steps: 226\n",
      "Episode: 807, Reward: 22.0, Epsilon: 0.0175, Steps: 596\n",
      "Episode: 808, Reward: 17.0, Epsilon: 0.0174, Steps: 297\n",
      "Target network updated at step 290000\n",
      "Episode: 809, Reward: 24.0, Epsilon: 0.0173, Steps: 831\n",
      "Target network updated at step 291000\n",
      "Episode: 810, Reward: 21.0, Epsilon: 0.0172, Steps: 703\n",
      "Target network updated at step 292000\n",
      "Episode: 811, Reward: 25.0, Epsilon: 0.0172, Steps: 783\n",
      "Episode: 812, Reward: 19.0, Epsilon: 0.0171, Steps: 280\n",
      "Target network updated at step 293000\n",
      "Episode: 813, Reward: 22.0, Epsilon: 0.0170, Steps: 499\n",
      "Episode: 814, Reward: 24.0, Epsilon: 0.0169, Steps: 416\n",
      "Episode: 815, Reward: 21.0, Epsilon: 0.0168, Steps: 345\n",
      "Target network updated at step 294000\n",
      "Episode: 816, Reward: 25.0, Epsilon: 0.0167, Steps: 468\n",
      "Episode: 817, Reward: 18.0, Epsilon: 0.0167, Steps: 427\n",
      "Target network updated at step 295000\n",
      "Episode: 818, Reward: 17.0, Epsilon: 0.0166, Steps: 544\n",
      "Episode: 819, Reward: 16.0, Epsilon: 0.0165, Steps: 281\n",
      "Episode: 820, Reward: 18.0, Epsilon: 0.0164, Steps: 425\n",
      "Target network updated at step 296000\n",
      "Episode: 821, Reward: 15.0, Epsilon: 0.0163, Steps: 320\n",
      "Episode: 822, Reward: 20.0, Epsilon: 0.0162, Steps: 312\n",
      "Episode: 823, Reward: 16.0, Epsilon: 0.0162, Steps: 365\n",
      "Target network updated at step 297000\n",
      "Episode: 824, Reward: 15.0, Epsilon: 0.0161, Steps: 703\n",
      "Target network updated at step 298000\n",
      "Episode: 825, Reward: 21.0, Epsilon: 0.0160, Steps: 539\n",
      "Episode: 826, Reward: 12.0, Epsilon: 0.0159, Steps: 700\n",
      "Target network updated at step 299000\n",
      "Episode: 827, Reward: 19.0, Epsilon: 0.0158, Steps: 609\n",
      "Episode: 828, Reward: 18.0, Epsilon: 0.0158, Steps: 401\n",
      "Target network updated at step 300000\n",
      "Episode: 829, Reward: 16.0, Epsilon: 0.0157, Steps: 327\n",
      "Episode: 830, Reward: 18.0, Epsilon: 0.0156, Steps: 719\n",
      "Target network updated at step 301000\n",
      "Episode: 831, Reward: 13.0, Epsilon: 0.0155, Steps: 289\n",
      "Episode: 832, Reward: 20.0, Epsilon: 0.0154, Steps: 545\n",
      "Target network updated at step 302000\n",
      "Episode: 833, Reward: 18.0, Epsilon: 0.0154, Steps: 719\n",
      "Episode: 834, Reward: 20.0, Epsilon: 0.0153, Steps: 385\n",
      "Target network updated at step 303000\n",
      "Episode: 835, Reward: 18.0, Epsilon: 0.0152, Steps: 497\n",
      "Episode: 836, Reward: 21.0, Epsilon: 0.0151, Steps: 441\n",
      "Target network updated at step 304000\n",
      "Episode: 837, Reward: 24.0, Epsilon: 0.0151, Steps: 735\n",
      "Target network updated at step 305000\n",
      "Episode: 838, Reward: 15.0, Epsilon: 0.0150, Steps: 673\n",
      "Episode: 839, Reward: 26.0, Epsilon: 0.0149, Steps: 332\n",
      "Episode: 840, Reward: 21.0, Epsilon: 0.0148, Steps: 335\n",
      "Target network updated at step 306000\n",
      "Episode: 841, Reward: 20.0, Epsilon: 0.0148, Steps: 719\n",
      "Target network updated at step 307000\n",
      "Episode: 842, Reward: 21.0, Epsilon: 0.0147, Steps: 703\n",
      "Episode: 843, Reward: 17.0, Epsilon: 0.0146, Steps: 313\n",
      "Target network updated at step 308000\n",
      "Episode: 844, Reward: 17.0, Epsilon: 0.0145, Steps: 837\n",
      "Episode: 845, Reward: 18.0, Epsilon: 0.0145, Steps: 473\n",
      "Target network updated at step 309000\n",
      "Episode: 846, Reward: 21.0, Epsilon: 0.0144, Steps: 484\n",
      "Target network updated at step 310000\n",
      "Episode: 847, Reward: 25.0, Epsilon: 0.0143, Steps: 762\n",
      "Episode: 848, Reward: 25.0, Epsilon: 0.0143, Steps: 349\n",
      "Episode: 849, Reward: 18.0, Epsilon: 0.0142, Steps: 353\n",
      "Target network updated at step 311000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 850, Reward: 25.0, Epsilon: 0.0141, Steps: 751\n",
      "Model saved at episode 850\n",
      "Target network updated at step 312000\n",
      "Episode: 851, Reward: 19.0, Epsilon: 0.0140, Steps: 719\n",
      "Episode: 852, Reward: 21.0, Epsilon: 0.0140, Steps: 488\n",
      "Target network updated at step 313000\n",
      "Episode: 853, Reward: 24.0, Epsilon: 0.0139, Steps: 327\n",
      "Episode: 854, Reward: 16.0, Epsilon: 0.0138, Steps: 473\n",
      "Target network updated at step 314000\n",
      "Episode: 855, Reward: 20.0, Epsilon: 0.0138, Steps: 695\n",
      "Target network updated at step 315000\n",
      "Episode: 856, Reward: 16.0, Epsilon: 0.0137, Steps: 703\n",
      "Episode: 857, Reward: 24.0, Epsilon: 0.0136, Steps: 767\n",
      "Target network updated at step 316000\n",
      "Episode: 858, Reward: 15.0, Epsilon: 0.0136, Steps: 225\n",
      "Episode: 859, Reward: 11.0, Epsilon: 0.0135, Steps: 377\n",
      "Target network updated at step 317000\n",
      "Episode: 860, Reward: 20.0, Epsilon: 0.0134, Steps: 633\n",
      "Episode: 861, Reward: 18.0, Epsilon: 0.0134, Steps: 282\n",
      "Episode: 862, Reward: 17.0, Epsilon: 0.0133, Steps: 465\n",
      "Target network updated at step 318000\n",
      "Episode: 863, Reward: 20.0, Epsilon: 0.0132, Steps: 269\n",
      "Episode: 864, Reward: 19.0, Epsilon: 0.0132, Steps: 338\n",
      "Target network updated at step 319000\n",
      "Episode: 865, Reward: 24.0, Epsilon: 0.0131, Steps: 783\n",
      "Episode: 866, Reward: 18.0, Epsilon: 0.0130, Steps: 429\n",
      "Episode: 867, Reward: 19.0, Epsilon: 0.0130, Steps: 284\n",
      "Target network updated at step 320000\n",
      "Episode: 868, Reward: 14.0, Epsilon: 0.0129, Steps: 783\n",
      "Target network updated at step 321000\n",
      "Episode: 869, Reward: 14.0, Epsilon: 0.0128, Steps: 767\n",
      "Episode: 870, Reward: 21.0, Epsilon: 0.0128, Steps: 389\n",
      "Target network updated at step 322000\n",
      "Episode: 871, Reward: 26.0, Epsilon: 0.0127, Steps: 863\n",
      "Episode: 872, Reward: 17.0, Epsilon: 0.0126, Steps: 281\n",
      "Target network updated at step 323000\n",
      "Episode: 873, Reward: 14.0, Epsilon: 0.0126, Steps: 719\n",
      "Target network updated at step 324000\n",
      "Episode: 874, Reward: 18.0, Epsilon: 0.0125, Steps: 368\n",
      "Episode: 875, Reward: 11.0, Epsilon: 0.0125, Steps: 298\n",
      "Episode: 876, Reward: 21.0, Epsilon: 0.0124, Steps: 373\n",
      "Target network updated at step 325000\n",
      "Episode: 877, Reward: 24.0, Epsilon: 0.0123, Steps: 345\n",
      "Episode: 878, Reward: 14.0, Epsilon: 0.0123, Steps: 267\n",
      "Episode: 879, Reward: 23.0, Epsilon: 0.0122, Steps: 377\n",
      "Target network updated at step 326000\n",
      "Episode: 880, Reward: 24.0, Epsilon: 0.0121, Steps: 521\n",
      "Episode: 881, Reward: 17.0, Epsilon: 0.0121, Steps: 389\n",
      "Target network updated at step 327000\n",
      "Episode: 882, Reward: 19.0, Epsilon: 0.0120, Steps: 661\n",
      "Episode: 883, Reward: 11.0, Epsilon: 0.0120, Steps: 329\n",
      "Target network updated at step 328000\n",
      "Episode: 884, Reward: 22.0, Epsilon: 0.0119, Steps: 369\n",
      "Episode: 885, Reward: 20.0, Epsilon: 0.0118, Steps: 719\n",
      "Target network updated at step 329000\n",
      "Episode: 886, Reward: 26.0, Epsilon: 0.0118, Steps: 433\n",
      "Episode: 887, Reward: 16.0, Epsilon: 0.0117, Steps: 544\n",
      "Target network updated at step 330000\n",
      "Episode: 888, Reward: 20.0, Epsilon: 0.0117, Steps: 462\n",
      "Episode: 889, Reward: 19.0, Epsilon: 0.0116, Steps: 393\n",
      "Target network updated at step 331000\n",
      "Episode: 890, Reward: 10.0, Epsilon: 0.0115, Steps: 735\n",
      "Target network updated at step 332000\n",
      "Episode: 891, Reward: 30.0, Epsilon: 0.0115, Steps: 799\n",
      "Episode: 892, Reward: 14.0, Epsilon: 0.0114, Steps: 537\n",
      "Target network updated at step 333000\n",
      "Episode: 893, Reward: 19.0, Epsilon: 0.0114, Steps: 383\n",
      "Episode: 894, Reward: 20.0, Epsilon: 0.0113, Steps: 639\n",
      "Episode: 895, Reward: 16.0, Epsilon: 0.0113, Steps: 303\n",
      "Target network updated at step 334000\n",
      "Episode: 896, Reward: 22.0, Epsilon: 0.0112, Steps: 550\n",
      "Target network updated at step 335000\n",
      "Episode: 897, Reward: 19.0, Epsilon: 0.0112, Steps: 593\n",
      "Episode: 898, Reward: 18.0, Epsilon: 0.0111, Steps: 408\n",
      "Episode: 899, Reward: 18.0, Epsilon: 0.0110, Steps: 324\n",
      "Target network updated at step 336000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900, Reward: 13.0, Epsilon: 0.0110, Steps: 577\n",
      "Model saved at episode 900\n",
      "Episode: 901, Reward: 29.0, Epsilon: 0.0109, Steps: 416\n",
      "Target network updated at step 337000\n",
      "Episode: 902, Reward: 14.0, Epsilon: 0.0109, Steps: 265\n",
      "Episode: 903, Reward: 12.0, Epsilon: 0.0108, Steps: 599\n",
      "Target network updated at step 338000\n",
      "Episode: 904, Reward: 11.0, Epsilon: 0.0108, Steps: 719\n",
      "Episode: 905, Reward: 21.0, Epsilon: 0.0107, Steps: 390\n",
      "Target network updated at step 339000\n",
      "Episode: 906, Reward: 22.0, Epsilon: 0.0107, Steps: 440\n",
      "Episode: 907, Reward: 18.0, Epsilon: 0.0106, Steps: 253\n",
      "Episode: 908, Reward: 14.0, Epsilon: 0.0106, Steps: 285\n",
      "Target network updated at step 340000\n",
      "Episode: 909, Reward: 26.0, Epsilon: 0.0105, Steps: 653\n",
      "Target network updated at step 341000\n",
      "Episode: 910, Reward: 24.0, Epsilon: 0.0104, Steps: 847\n",
      "Episode: 911, Reward: 19.0, Epsilon: 0.0104, Steps: 281\n",
      "Target network updated at step 342000\n",
      "Episode: 912, Reward: 25.0, Epsilon: 0.0103, Steps: 496\n",
      "Episode: 913, Reward: 24.0, Epsilon: 0.0103, Steps: 607\n",
      "Target network updated at step 343000\n",
      "Episode: 914, Reward: 25.0, Epsilon: 0.0102, Steps: 719\n",
      "Episode: 915, Reward: 22.0, Epsilon: 0.0102, Steps: 411\n",
      "Target network updated at step 344000\n",
      "Episode: 916, Reward: 23.0, Epsilon: 0.0101, Steps: 649\n",
      "Episode: 917, Reward: 22.0, Epsilon: 0.0101, Steps: 409\n",
      "Target network updated at step 345000\n",
      "Episode: 918, Reward: 27.0, Epsilon: 0.0100, Steps: 457\n",
      "Target network updated at step 346000\n",
      "Episode: 919, Reward: 18.0, Epsilon: 0.0100, Steps: 751\n",
      "Episode: 920, Reward: 17.0, Epsilon: 0.0100, Steps: 303\n",
      "Episode: 921, Reward: 20.0, Epsilon: 0.0100, Steps: 600\n",
      "Target network updated at step 347000\n",
      "Episode: 922, Reward: 22.0, Epsilon: 0.0100, Steps: 751\n",
      "Target network updated at step 348000\n",
      "Episode: 923, Reward: 20.0, Epsilon: 0.0100, Steps: 751\n",
      "Target network updated at step 349000\n",
      "Episode: 924, Reward: 18.0, Epsilon: 0.0100, Steps: 719\n",
      "Episode: 925, Reward: 12.0, Epsilon: 0.0100, Steps: 553\n",
      "Target network updated at step 350000\n",
      "Episode: 926, Reward: 17.0, Epsilon: 0.0100, Steps: 719\n",
      "Episode: 927, Reward: 19.0, Epsilon: 0.0100, Steps: 484\n",
      "Target network updated at step 351000\n",
      "Episode: 928, Reward: 16.0, Epsilon: 0.0100, Steps: 307\n",
      "Episode: 929, Reward: 22.0, Epsilon: 0.0100, Steps: 641\n",
      "Target network updated at step 352000\n",
      "Episode: 930, Reward: 13.0, Epsilon: 0.0100, Steps: 240\n",
      "Episode: 931, Reward: 18.0, Epsilon: 0.0100, Steps: 767\n",
      "Target network updated at step 353000\n",
      "Episode: 932, Reward: 12.0, Epsilon: 0.0100, Steps: 751\n",
      "Target network updated at step 354000\n",
      "Episode: 933, Reward: 17.0, Epsilon: 0.0100, Steps: 445\n",
      "Episode: 934, Reward: 18.0, Epsilon: 0.0100, Steps: 414\n",
      "Episode: 935, Reward: 19.0, Epsilon: 0.0100, Steps: 329\n",
      "Target network updated at step 355000\n",
      "Episode: 936, Reward: 19.0, Epsilon: 0.0100, Steps: 703\n",
      "Episode: 937, Reward: 15.0, Epsilon: 0.0100, Steps: 221\n",
      "Episode: 938, Reward: 12.0, Epsilon: 0.0100, Steps: 235\n",
      "Target network updated at step 356000\n",
      "Episode: 939, Reward: 23.0, Epsilon: 0.0100, Steps: 564\n",
      "Episode: 940, Reward: 16.0, Epsilon: 0.0100, Steps: 294\n",
      "Target network updated at step 357000\n",
      "Episode: 941, Reward: 15.0, Epsilon: 0.0100, Steps: 703\n",
      "Episode: 942, Reward: 17.0, Epsilon: 0.0100, Steps: 235\n",
      "Target network updated at step 358000\n",
      "Episode: 943, Reward: 19.0, Epsilon: 0.0100, Steps: 367\n",
      "Episode: 944, Reward: 15.0, Epsilon: 0.0100, Steps: 579\n",
      "Episode: 945, Reward: 16.0, Epsilon: 0.0100, Steps: 233\n",
      "Target network updated at step 359000\n",
      "Episode: 946, Reward: 12.0, Epsilon: 0.0100, Steps: 239\n",
      "Episode: 947, Reward: 12.0, Epsilon: 0.0100, Steps: 498\n",
      "Episode: 948, Reward: 15.0, Epsilon: 0.0100, Steps: 239\n",
      "Target network updated at step 360000\n",
      "Episode: 949, Reward: 10.0, Epsilon: 0.0100, Steps: 561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 950, Reward: 14.0, Epsilon: 0.0100, Steps: 286\n",
      "Model saved at episode 950\n",
      "Target network updated at step 361000\n",
      "Episode: 951, Reward: 16.0, Epsilon: 0.0100, Steps: 646\n",
      "Target network updated at step 362000\n",
      "Episode: 952, Reward: 23.0, Epsilon: 0.0100, Steps: 635\n",
      "Episode: 953, Reward: 19.0, Epsilon: 0.0100, Steps: 475\n",
      "Target network updated at step 363000\n",
      "Episode: 954, Reward: 20.0, Epsilon: 0.0100, Steps: 539\n",
      "Episode: 955, Reward: 17.0, Epsilon: 0.0100, Steps: 719\n",
      "Target network updated at step 364000\n",
      "Episode: 956, Reward: 21.0, Epsilon: 0.0100, Steps: 456\n",
      "Episode: 957, Reward: 17.0, Epsilon: 0.0100, Steps: 280\n",
      "Episode: 958, Reward: 24.0, Epsilon: 0.0100, Steps: 347\n",
      "Target network updated at step 365000\n",
      "Episode: 959, Reward: 15.0, Epsilon: 0.0100, Steps: 448\n",
      "Episode: 960, Reward: 19.0, Epsilon: 0.0100, Steps: 288\n",
      "Target network updated at step 366000\n",
      "Episode: 961, Reward: 20.0, Epsilon: 0.0100, Steps: 473\n",
      "Episode: 962, Reward: 20.0, Epsilon: 0.0100, Steps: 301\n",
      "Episode: 963, Reward: 17.0, Epsilon: 0.0100, Steps: 281\n",
      "Target network updated at step 367000\n",
      "Episode: 964, Reward: 25.0, Epsilon: 0.0100, Steps: 429\n",
      "Episode: 965, Reward: 19.0, Epsilon: 0.0100, Steps: 703\n",
      "Target network updated at step 368000\n",
      "Episode: 966, Reward: 26.0, Epsilon: 0.0100, Steps: 713\n",
      "Target network updated at step 369000\n",
      "Episode: 967, Reward: 11.0, Epsilon: 0.0100, Steps: 703\n",
      "Episode: 968, Reward: 13.0, Epsilon: 0.0100, Steps: 703\n",
      "Target network updated at step 370000\n",
      "Episode: 969, Reward: 23.0, Epsilon: 0.0100, Steps: 457\n",
      "Episode: 970, Reward: 13.0, Epsilon: 0.0100, Steps: 393\n",
      "Target network updated at step 371000\n",
      "Episode: 971, Reward: 12.0, Epsilon: 0.0100, Steps: 370\n",
      "Episode: 972, Reward: 20.0, Epsilon: 0.0100, Steps: 270\n",
      "Episode: 973, Reward: 17.0, Epsilon: 0.0100, Steps: 267\n",
      "Episode: 974, Reward: 19.0, Epsilon: 0.0100, Steps: 320\n",
      "Target network updated at step 372000\n",
      "Episode: 975, Reward: 14.0, Epsilon: 0.0100, Steps: 285\n",
      "Episode: 976, Reward: 16.0, Epsilon: 0.0100, Steps: 269\n",
      "Episode: 977, Reward: 24.0, Epsilon: 0.0100, Steps: 383\n",
      "Target network updated at step 373000\n",
      "Episode: 978, Reward: 19.0, Epsilon: 0.0100, Steps: 545\n",
      "Episode: 979, Reward: 18.0, Epsilon: 0.0100, Steps: 295\n",
      "Target network updated at step 374000\n",
      "Episode: 980, Reward: 14.0, Epsilon: 0.0100, Steps: 365\n",
      "Episode: 981, Reward: 17.0, Epsilon: 0.0100, Steps: 767\n",
      "Target network updated at step 375000\n",
      "Episode: 982, Reward: 16.0, Epsilon: 0.0100, Steps: 703\n",
      "Target network updated at step 376000\n",
      "Episode: 983, Reward: 17.0, Epsilon: 0.0100, Steps: 767\n",
      "Target network updated at step 377000\n",
      "Episode: 984, Reward: 22.0, Epsilon: 0.0100, Steps: 767\n",
      "Episode: 985, Reward: 12.0, Epsilon: 0.0100, Steps: 625\n",
      "Target network updated at step 378000\n",
      "Episode: 986, Reward: 20.0, Epsilon: 0.0100, Steps: 521\n",
      "Episode: 987, Reward: 16.0, Epsilon: 0.0100, Steps: 393\n",
      "Target network updated at step 379000\n",
      "Episode: 988, Reward: 19.0, Epsilon: 0.0100, Steps: 745\n",
      "Target network updated at step 380000\n",
      "Episode: 989, Reward: 19.0, Epsilon: 0.0100, Steps: 753\n",
      "Episode: 990, Reward: 15.0, Epsilon: 0.0100, Steps: 745\n",
      "Target network updated at step 381000\n",
      "Episode: 991, Reward: 11.0, Epsilon: 0.0100, Steps: 433\n",
      "Target network updated at step 382000\n",
      "Episode: 992, Reward: 20.0, Epsilon: 0.0100, Steps: 735\n",
      "Target network updated at step 383000\n",
      "Episode: 993, Reward: 21.0, Epsilon: 0.0100, Steps: 927\n",
      "Episode: 994, Reward: 12.0, Epsilon: 0.0100, Steps: 482\n",
      "Target network updated at step 384000\n",
      "Episode: 995, Reward: 14.0, Epsilon: 0.0100, Steps: 588\n",
      "Episode: 996, Reward: 21.0, Epsilon: 0.0100, Steps: 318\n",
      "Target network updated at step 385000\n",
      "Episode: 997, Reward: 12.0, Epsilon: 0.0100, Steps: 703\n",
      "Episode: 998, Reward: 19.0, Epsilon: 0.0100, Steps: 229\n",
      "Episode: 999, Reward: 20.0, Epsilon: 0.0100, Steps: 358\n",
      "Target network updated at step 386000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1000, Reward: 20.0, Epsilon: 0.0100, Steps: 751\n",
      "Model saved at episode 1000\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB frame to grayscale and keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Keep original dimensions\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame\n",
    "\n",
    "# Building the CNN model for Q-learning\n",
    "def build_model(action_size):\n",
    "    \"\"\"Build a CNN model for Deep Q-Learning\"\"\"\n",
    "    model = Sequential()\n",
    "    # Input shape: grayscale image of 210x160 (210, 160, 1)\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))  # Output layer with one node per action\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model\n",
    "\n",
    "# Experience replay memory\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size)\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.random() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose random action\n",
    "    else:\n",
    "        # Exploit: choose best action\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)  # Choose action with highest Q-value\n",
    "\n",
    "# Training function\n",
    "def train_dqn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=10000,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a DQN model on Frogger\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create main and target models\n",
    "    main_model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(main_model.get_weights())  # Initialize target with same weights\n",
    "    \n",
    "    # Create memory for experience replay\n",
    "    memory = create_memory(capacity=memory_capacity)\n",
    "    \n",
    "    # Training metrics\n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Model saving directory\n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, episodes + 1):\n",
    "        # Reset environment and get initial state\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension: (120, 160, 1)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = epsilon_greedy_action(main_model, state, epsilon, action_size)\n",
    "            \n",
    "            # Take action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)  # Add channel dimension\n",
    "            \n",
    "            # Store experience in memory\n",
    "            add_to_memory(memory, state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Train with experience replay if memory has enough samples\n",
    "            if len(memory) > batch_size:\n",
    "                # Sample batch from memory\n",
    "                minibatch = sample_from_memory(memory, batch_size)\n",
    "                \n",
    "                # Prepare batch for training\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                target_q_values = main_model.predict(states, verbose=0)\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        target_q_values[i, actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target_q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "                \n",
    "                # Train the model\n",
    "                main_model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "            \n",
    "            # Update target network periodically\n",
    "            if total_steps % update_target_freq == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"Target network updated at step {total_steps}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}, Steps: {step+1}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % save_freq == 0:\n",
    "            main_model.save(f\"{save_dir}/frogger_dqn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    # Save final model\n",
    "    main_model.save(f\"{save_dir}/frogger_dqn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return main_model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as needed\n",
    "    train_dqn(episodes=1000,\n",
    "              max_steps=10000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.01, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=1000,\n",
    "              memory_capacity=50000,\n",
    "              save_freq=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69dd31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB frame to grayscale and keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Keep original dimensions\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a178f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"Build a CNN model for Deep Q-Learning\"\"\"\n",
    "    model = Sequential()\n",
    "    # Input shape: grayscale image of 210x160 (210, 160, 1)\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))  # Output layer with one node per action\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4886457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sspindt\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 - Total Reward: 23.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23764\\213839366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Assuming you have saved your trained model in 'frogger_model/frogger_dqn_final.h5'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mrun_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ALE/Frogger-v5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'frogger_model/frogger_dqn_final.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23764\\213839366.py\u001b[0m in \u001b[0;36mrun_trained_model\u001b[1;34m(env_name, model_path, episodes)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Take the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mnext_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\gymnasium\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\shimmy\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[0mis_terminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_truncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mis_truncated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_truncated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_trained_model(env_name, model_path, episodes=5):\n",
    "    # Initialize the environment\n",
    "    env = gym.make(env_name, render_mode='human' )\n",
    "    model = build_model(env.action_space.n)\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Render the environment to see the agent in action\n",
    "            time.sleep(0.01)  # Slow down simulation for better visualization\n",
    "\n",
    "            # Predict action from the model\n",
    "            q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "            action = np.argmax(q_values[0])  # Choose action with highest predicted Q-value\n",
    "\n",
    "            # Take the action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)\n",
    "\n",
    "            # Update the current state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode + 1} - Total Reward: {episode_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Assuming you have saved your trained model in 'frogger_model/frogger_dqn_final.h5'\n",
    "run_trained_model(env_name='ALE/Frogger-v5', model_path='frogger_model/frogger_dqn_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16d29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
