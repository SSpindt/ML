{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37d355c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d5c04dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_frame(frame):\n",
    "    \"\"\"Convert RGB data to grayscale. Keep original dimensions (210x160)\"\"\"\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    normalized_frame = gray_frame / 255.0  # Normalize pixel values\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b98a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    \"\"\"CNN architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(210, 160, 1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.00025))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f20112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System for progressively saving weights as the CNN trains\n",
    "def create_memory(capacity=100000):\n",
    "    \"\"\"Create a memory buffer for experience replay\"\"\"\n",
    "    return deque(maxlen=capacity)\n",
    "\n",
    "def add_to_memory(memory, state, action, reward, next_state, done):\n",
    "    \"\"\"Add experience to memory\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_from_memory(memory, batch_size):\n",
    "    \"\"\"Sample random batch from memory\"\"\"\n",
    "    return random.sample(memory, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon-greedy policy -- balances exploration and exploitation\n",
    "def epsilon_greedy_action(model, state, epsilon, action_size):\n",
    "    \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "    if np.random.random() <= epsilon:\n",
    "        return random.randrange(action_size)  # Explore: choose random action\n",
    "    else:\n",
    "        # Exploit: choose best action\n",
    "        q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "        return np.argmax(q_values)  # Choose action with highest Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9affe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_dqn(episodes=10000, \n",
    "              max_steps=50000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.1, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=10000,\n",
    "              memory_capacity=100000,\n",
    "              save_freq=100):\n",
    "    \"\"\"Train a DQN model on Frogger\"\"\"\n",
    "    # Create environment\n",
    "    env = gym.make('ALE/Frogger-v5')\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    # Create main and target models\n",
    "    main_model = build_model(action_size)\n",
    "    target_model = build_model(action_size)\n",
    "    target_model.set_weights(main_model.get_weights())  # Initialize target with same weights\n",
    "    \n",
    "    # Create memory for experience replay\n",
    "    memory = create_memory(capacity=memory_capacity)\n",
    "    \n",
    "    # Training metrics\n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    \n",
    "    # Model saving directory\n",
    "    save_dir = \"frogger_model\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(1, episodes + 1):\n",
    "        # Reset environment and get initial state\n",
    "        frame, info = env.reset()\n",
    "        state = preprocess_frame(frame)\n",
    "        state = np.expand_dims(state, axis=-1)  # Add channel dimension: (120, 160, 1)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = epsilon_greedy_action(main_model, state, epsilon, action_size)\n",
    "            \n",
    "            # Take action\n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Prepare next state\n",
    "            next_state = preprocess_frame(next_frame)\n",
    "            next_state = np.expand_dims(next_state, axis=-1)  # Add channel dimension\n",
    "            \n",
    "            # Store experience in memory\n",
    "            add_to_memory(memory, state, action, reward, next_state, done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Train with experience replay if memory has enough samples\n",
    "            if len(memory) > batch_size:\n",
    "                # Sample batch from memory\n",
    "                minibatch = sample_from_memory(memory, batch_size)\n",
    "                \n",
    "                # Prepare batch for training\n",
    "                states = np.array([experience[0] for experience in minibatch])\n",
    "                actions = np.array([experience[1] for experience in minibatch])\n",
    "                rewards = np.array([experience[2] for experience in minibatch])\n",
    "                next_states = np.array([experience[3] for experience in minibatch])\n",
    "                dones = np.array([experience[4] for experience in minibatch])\n",
    "                \n",
    "                # Calculate target Q values\n",
    "                target_q_values = main_model.predict(states, verbose=0)\n",
    "                next_q_values = target_model.predict(next_states, verbose=0)\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    if dones[i]:\n",
    "                        target_q_values[i, actions[i]] = rewards[i]\n",
    "                    else:\n",
    "                        target_q_values[i, actions[i]] = rewards[i] + gamma * np.max(next_q_values[i])\n",
    "                \n",
    "                # Train the model\n",
    "                main_model.fit(states, target_q_values, epochs=1, verbose=0)\n",
    "            \n",
    "            # Update target network periodically\n",
    "            if total_steps % update_target_freq == 0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "                print(f\"Target network updated at step {total_steps}\")\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if epsilon > epsilon_end:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "        # Print episode stats\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}, Epsilon: {epsilon:.4f}, Steps: {step+1}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if episode % save_freq == 0:\n",
    "            main_model.save(f\"{save_dir}/frogger_dqn_episode_{episode}.h5\")\n",
    "            print(f\"Model saved at episode {episode}\")\n",
    "    \n",
    "    # Save final model\n",
    "    main_model.save(f\"{save_dir}/frogger_dqn_final.h5\")\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    env.close()\n",
    "    return main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e63196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 10.0, Epsilon: 0.9950, Steps: 271\n",
      "Episode: 2, Reward: 7.0, Epsilon: 0.9900, Steps: 232\n",
      "Episode: 3, Reward: 8.0, Epsilon: 0.9851, Steps: 277\n",
      "Target network updated at step 1000\n",
      "Episode: 4, Reward: 9.0, Epsilon: 0.9801, Steps: 290\n",
      "Episode: 5, Reward: 8.0, Epsilon: 0.9752, Steps: 258\n",
      "Episode: 6, Reward: 10.0, Epsilon: 0.9704, Steps: 307\n",
      "Episode: 7, Reward: 8.0, Epsilon: 0.9655, Steps: 248\n",
      "Target network updated at step 2000\n",
      "Episode: 8, Reward: 9.0, Epsilon: 0.9607, Steps: 306\n",
      "Episode: 9, Reward: 7.0, Epsilon: 0.9559, Steps: 262\n",
      "Episode: 10, Reward: 13.0, Epsilon: 0.9511, Steps: 281\n",
      "Episode: 11, Reward: 11.0, Epsilon: 0.9464, Steps: 248\n",
      "Target network updated at step 3000\n",
      "Episode: 12, Reward: 9.0, Epsilon: 0.9416, Steps: 217\n",
      "Episode: 13, Reward: 7.0, Epsilon: 0.9369, Steps: 273\n",
      "Episode: 14, Reward: 9.0, Epsilon: 0.9322, Steps: 325\n",
      "Target network updated at step 4000\n",
      "Episode: 15, Reward: 8.0, Epsilon: 0.9276, Steps: 226\n",
      "Episode: 16, Reward: 12.0, Epsilon: 0.9229, Steps: 281\n",
      "Episode: 17, Reward: 8.0, Epsilon: 0.9183, Steps: 265\n",
      "Episode: 18, Reward: 10.0, Epsilon: 0.9137, Steps: 248\n",
      "Target network updated at step 5000\n",
      "Episode: 19, Reward: 13.0, Epsilon: 0.9092, Steps: 281\n",
      "Episode: 20, Reward: 9.0, Epsilon: 0.9046, Steps: 246\n",
      "Episode: 21, Reward: 8.0, Epsilon: 0.9001, Steps: 289\n",
      "Episode: 22, Reward: 12.0, Epsilon: 0.8956, Steps: 300\n",
      "Target network updated at step 6000\n",
      "Episode: 23, Reward: 10.0, Epsilon: 0.8911, Steps: 354\n",
      "Episode: 24, Reward: 9.0, Epsilon: 0.8867, Steps: 262\n",
      "Episode: 25, Reward: 6.0, Epsilon: 0.8822, Steps: 233\n",
      "Target network updated at step 7000\n",
      "Episode: 26, Reward: 10.0, Epsilon: 0.8778, Steps: 303\n",
      "Episode: 27, Reward: 10.0, Epsilon: 0.8734, Steps: 265\n",
      "Episode: 28, Reward: 10.0, Epsilon: 0.8691, Steps: 374\n",
      "Target network updated at step 8000\n",
      "Episode: 29, Reward: 13.0, Epsilon: 0.8647, Steps: 284\n",
      "Episode: 30, Reward: 11.0, Epsilon: 0.8604, Steps: 268\n",
      "Episode: 31, Reward: 17.0, Epsilon: 0.8561, Steps: 324\n",
      "Episode: 32, Reward: 7.0, Epsilon: 0.8518, Steps: 238\n",
      "Target network updated at step 9000\n",
      "Episode: 33, Reward: 10.0, Epsilon: 0.8475, Steps: 261\n",
      "Episode: 34, Reward: 11.0, Epsilon: 0.8433, Steps: 281\n",
      "Episode: 35, Reward: 8.0, Epsilon: 0.8391, Steps: 297\n",
      "Target network updated at step 10000\n",
      "Episode: 36, Reward: 9.0, Epsilon: 0.8349, Steps: 378\n",
      "Episode: 37, Reward: 8.0, Epsilon: 0.8307, Steps: 220\n",
      "Episode: 38, Reward: 11.0, Epsilon: 0.8266, Steps: 295\n",
      "Episode: 39, Reward: 9.0, Epsilon: 0.8224, Steps: 290\n",
      "Target network updated at step 11000\n",
      "Episode: 40, Reward: 10.0, Epsilon: 0.8183, Steps: 255\n",
      "Episode: 41, Reward: 12.0, Epsilon: 0.8142, Steps: 276\n",
      "Episode: 42, Reward: 9.0, Epsilon: 0.8102, Steps: 298\n",
      "Episode: 43, Reward: 13.0, Epsilon: 0.8061, Steps: 273\n",
      "Target network updated at step 12000\n",
      "Episode: 44, Reward: 12.0, Epsilon: 0.8021, Steps: 279\n",
      "Episode: 45, Reward: 9.0, Epsilon: 0.7981, Steps: 297\n",
      "Episode: 46, Reward: 6.0, Epsilon: 0.7941, Steps: 270\n",
      "Target network updated at step 13000\n",
      "Episode: 47, Reward: 10.0, Epsilon: 0.7901, Steps: 251\n",
      "Episode: 48, Reward: 10.0, Epsilon: 0.7862, Steps: 270\n",
      "Episode: 49, Reward: 12.0, Epsilon: 0.7822, Steps: 290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Reward: 12.0, Epsilon: 0.7783, Steps: 279\n",
      "Model saved at episode 50\n",
      "Target network updated at step 14000\n",
      "Episode: 51, Reward: 13.0, Epsilon: 0.7744, Steps: 270\n",
      "Episode: 52, Reward: 8.0, Epsilon: 0.7705, Steps: 236\n",
      "Episode: 53, Reward: 11.0, Epsilon: 0.7667, Steps: 285\n",
      "Episode: 54, Reward: 10.0, Epsilon: 0.7629, Steps: 259\n",
      "Target network updated at step 15000\n",
      "Episode: 55, Reward: 8.0, Epsilon: 0.7590, Steps: 229\n",
      "Episode: 56, Reward: 7.0, Epsilon: 0.7553, Steps: 249\n",
      "Episode: 57, Reward: 14.0, Epsilon: 0.7515, Steps: 275\n",
      "Episode: 58, Reward: 11.0, Epsilon: 0.7477, Steps: 245\n",
      "Target network updated at step 16000\n",
      "Episode: 59, Reward: 10.0, Epsilon: 0.7440, Steps: 320\n",
      "Episode: 60, Reward: 9.0, Epsilon: 0.7403, Steps: 279\n",
      "Episode: 61, Reward: 8.0, Epsilon: 0.7366, Steps: 210\n",
      "Target network updated at step 17000\n",
      "Episode: 62, Reward: 10.0, Epsilon: 0.7329, Steps: 308\n",
      "Episode: 63, Reward: 11.0, Epsilon: 0.7292, Steps: 260\n",
      "Episode: 64, Reward: 8.0, Epsilon: 0.7256, Steps: 252\n",
      "Episode: 65, Reward: 13.0, Epsilon: 0.7219, Steps: 305\n",
      "Target network updated at step 18000\n",
      "Episode: 66, Reward: 12.0, Epsilon: 0.7183, Steps: 317\n",
      "Episode: 67, Reward: 17.0, Epsilon: 0.7147, Steps: 303\n",
      "Episode: 68, Reward: 14.0, Epsilon: 0.7112, Steps: 272\n",
      "Target network updated at step 19000\n",
      "Episode: 69, Reward: 10.0, Epsilon: 0.7076, Steps: 310\n",
      "Episode: 70, Reward: 10.0, Epsilon: 0.7041, Steps: 274\n",
      "Episode: 71, Reward: 9.0, Epsilon: 0.7005, Steps: 226\n",
      "Episode: 72, Reward: 10.0, Epsilon: 0.6970, Steps: 297\n",
      "Target network updated at step 20000\n",
      "Episode: 73, Reward: 13.0, Epsilon: 0.6936, Steps: 278\n",
      "Episode: 74, Reward: 11.0, Epsilon: 0.6901, Steps: 271\n",
      "Episode: 75, Reward: 12.0, Epsilon: 0.6866, Steps: 311\n",
      "Target network updated at step 21000\n",
      "Episode: 76, Reward: 13.0, Epsilon: 0.6832, Steps: 298\n",
      "Episode: 77, Reward: 9.0, Epsilon: 0.6798, Steps: 230\n",
      "Episode: 78, Reward: 11.0, Epsilon: 0.6764, Steps: 228\n",
      "Episode: 79, Reward: 12.0, Epsilon: 0.6730, Steps: 355\n",
      "Target network updated at step 22000\n",
      "Episode: 80, Reward: 10.0, Epsilon: 0.6696, Steps: 248\n",
      "Episode: 81, Reward: 10.0, Epsilon: 0.6663, Steps: 229\n",
      "Episode: 82, Reward: 19.0, Epsilon: 0.6630, Steps: 273\n",
      "Episode: 83, Reward: 10.0, Epsilon: 0.6597, Steps: 270\n",
      "Target network updated at step 23000\n",
      "Episode: 84, Reward: 15.0, Epsilon: 0.6564, Steps: 335\n",
      "Episode: 85, Reward: 8.0, Epsilon: 0.6531, Steps: 219\n",
      "Episode: 86, Reward: 11.0, Epsilon: 0.6498, Steps: 266\n",
      "Episode: 87, Reward: 9.0, Epsilon: 0.6466, Steps: 281\n",
      "Target network updated at step 24000\n",
      "Episode: 88, Reward: 12.0, Epsilon: 0.6433, Steps: 278\n",
      "Episode: 89, Reward: 12.0, Epsilon: 0.6401, Steps: 312\n",
      "Episode: 90, Reward: 9.0, Epsilon: 0.6369, Steps: 237\n",
      "Target network updated at step 25000\n",
      "Episode: 91, Reward: 10.0, Epsilon: 0.6337, Steps: 234\n",
      "Episode: 92, Reward: 14.0, Epsilon: 0.6306, Steps: 310\n",
      "Episode: 93, Reward: 16.0, Epsilon: 0.6274, Steps: 288\n",
      "Episode: 94, Reward: 12.0, Epsilon: 0.6243, Steps: 277\n",
      "Target network updated at step 26000\n",
      "Episode: 95, Reward: 11.0, Epsilon: 0.6211, Steps: 274\n",
      "Episode: 96, Reward: 18.0, Epsilon: 0.6180, Steps: 312\n",
      "Episode: 97, Reward: 12.0, Epsilon: 0.6149, Steps: 289\n",
      "Target network updated at step 27000\n",
      "Episode: 98, Reward: 17.0, Epsilon: 0.6119, Steps: 342\n",
      "Episode: 99, Reward: 11.0, Epsilon: 0.6088, Steps: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, Reward: 8.0, Epsilon: 0.6058, Steps: 233\n",
      "Model saved at episode 100\n",
      "Episode: 101, Reward: 16.0, Epsilon: 0.6027, Steps: 287\n",
      "Target network updated at step 28000\n",
      "Episode: 102, Reward: 11.0, Epsilon: 0.5997, Steps: 318\n",
      "Episode: 103, Reward: 12.0, Epsilon: 0.5967, Steps: 336\n",
      "Episode: 104, Reward: 12.0, Epsilon: 0.5937, Steps: 257\n",
      "Target network updated at step 29000\n",
      "Episode: 105, Reward: 11.0, Epsilon: 0.5908, Steps: 305\n",
      "Episode: 106, Reward: 15.0, Epsilon: 0.5878, Steps: 279\n",
      "Episode: 107, Reward: 11.0, Epsilon: 0.5849, Steps: 306\n",
      "Episode: 108, Reward: 14.0, Epsilon: 0.5820, Steps: 311\n",
      "Target network updated at step 30000\n",
      "Episode: 109, Reward: 14.0, Epsilon: 0.5790, Steps: 286\n",
      "Episode: 110, Reward: 10.0, Epsilon: 0.5762, Steps: 229\n",
      "Episode: 111, Reward: 13.0, Epsilon: 0.5733, Steps: 273\n",
      "Target network updated at step 31000\n",
      "Episode: 112, Reward: 18.0, Epsilon: 0.5704, Steps: 310\n",
      "Episode: 113, Reward: 16.0, Epsilon: 0.5676, Steps: 345\n",
      "Episode: 114, Reward: 15.0, Epsilon: 0.5647, Steps: 279\n",
      "Episode: 115, Reward: 13.0, Epsilon: 0.5619, Steps: 265\n",
      "Target network updated at step 32000\n",
      "Episode: 116, Reward: 12.0, Epsilon: 0.5591, Steps: 262\n",
      "Episode: 117, Reward: 17.0, Epsilon: 0.5563, Steps: 333\n",
      "Episode: 118, Reward: 14.0, Epsilon: 0.5535, Steps: 314\n",
      "Target network updated at step 33000\n",
      "Episode: 119, Reward: 14.0, Epsilon: 0.5507, Steps: 318\n",
      "Episode: 120, Reward: 11.0, Epsilon: 0.5480, Steps: 224\n",
      "Episode: 121, Reward: 10.0, Epsilon: 0.5452, Steps: 279\n",
      "Episode: 122, Reward: 11.0, Epsilon: 0.5425, Steps: 275\n",
      "Target network updated at step 34000\n",
      "Episode: 123, Reward: 10.0, Epsilon: 0.5398, Steps: 298\n",
      "Episode: 124, Reward: 15.0, Epsilon: 0.5371, Steps: 312\n",
      "Episode: 125, Reward: 16.0, Epsilon: 0.5344, Steps: 287\n",
      "Target network updated at step 35000\n",
      "Episode: 126, Reward: 12.0, Epsilon: 0.5318, Steps: 265\n",
      "Episode: 127, Reward: 14.0, Epsilon: 0.5291, Steps: 278\n",
      "Episode: 128, Reward: 17.0, Epsilon: 0.5264, Steps: 325\n",
      "Target network updated at step 36000\n",
      "Episode: 129, Reward: 18.0, Epsilon: 0.5238, Steps: 286\n",
      "Episode: 130, Reward: 17.0, Epsilon: 0.5212, Steps: 303\n",
      "Episode: 131, Reward: 10.0, Epsilon: 0.5186, Steps: 321\n",
      "Target network updated at step 37000\n",
      "Episode: 132, Reward: 17.0, Epsilon: 0.5160, Steps: 399\n",
      "Episode: 133, Reward: 10.0, Epsilon: 0.5134, Steps: 307\n",
      "Episode: 134, Reward: 9.0, Epsilon: 0.5108, Steps: 239\n",
      "Episode: 135, Reward: 15.0, Epsilon: 0.5083, Steps: 316\n",
      "Target network updated at step 38000\n",
      "Episode: 136, Reward: 13.0, Epsilon: 0.5058, Steps: 270\n",
      "Episode: 137, Reward: 15.0, Epsilon: 0.5032, Steps: 350\n",
      "Episode: 138, Reward: 16.0, Epsilon: 0.5007, Steps: 356\n",
      "Target network updated at step 39000\n",
      "Episode: 139, Reward: 13.0, Epsilon: 0.4982, Steps: 254\n",
      "Episode: 140, Reward: 10.0, Epsilon: 0.4957, Steps: 264\n",
      "Episode: 141, Reward: 17.0, Epsilon: 0.4932, Steps: 403\n",
      "Target network updated at step 40000\n",
      "Episode: 142, Reward: 9.0, Epsilon: 0.4908, Steps: 267\n",
      "Episode: 143, Reward: 11.0, Epsilon: 0.4883, Steps: 330\n",
      "Episode: 144, Reward: 14.0, Epsilon: 0.4859, Steps: 277\n",
      "Episode: 145, Reward: 15.0, Epsilon: 0.4834, Steps: 318\n",
      "Target network updated at step 41000\n",
      "Episode: 146, Reward: 16.0, Epsilon: 0.4810, Steps: 365\n",
      "Episode: 147, Reward: 12.0, Epsilon: 0.4786, Steps: 269\n",
      "Target network updated at step 42000\n",
      "Episode: 148, Reward: 17.0, Epsilon: 0.4762, Steps: 390\n",
      "Episode: 149, Reward: 15.0, Epsilon: 0.4738, Steps: 308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 150, Reward: 18.0, Epsilon: 0.4715, Steps: 321\n",
      "Model saved at episode 150\n",
      "Episode: 151, Reward: 19.0, Epsilon: 0.4691, Steps: 327\n",
      "Target network updated at step 43000\n",
      "Episode: 152, Reward: 14.0, Epsilon: 0.4668, Steps: 335\n",
      "Episode: 153, Reward: 12.0, Epsilon: 0.4644, Steps: 327\n",
      "Episode: 154, Reward: 16.0, Epsilon: 0.4621, Steps: 313\n",
      "Target network updated at step 44000\n",
      "Episode: 155, Reward: 18.0, Epsilon: 0.4598, Steps: 352\n",
      "Episode: 156, Reward: 14.0, Epsilon: 0.4575, Steps: 309\n",
      "Episode: 157, Reward: 9.0, Epsilon: 0.4552, Steps: 302\n",
      "Target network updated at step 45000\n",
      "Episode: 158, Reward: 11.0, Epsilon: 0.4529, Steps: 307\n",
      "Episode: 159, Reward: 11.0, Epsilon: 0.4507, Steps: 236\n",
      "Episode: 160, Reward: 11.0, Epsilon: 0.4484, Steps: 300\n",
      "Target network updated at step 46000\n",
      "Episode: 161, Reward: 18.0, Epsilon: 0.4462, Steps: 389\n",
      "Episode: 162, Reward: 11.0, Epsilon: 0.4440, Steps: 265\n",
      "Episode: 163, Reward: 12.0, Epsilon: 0.4417, Steps: 319\n",
      "Episode: 164, Reward: 10.0, Epsilon: 0.4395, Steps: 229\n",
      "Target network updated at step 47000\n",
      "Episode: 165, Reward: 15.0, Epsilon: 0.4373, Steps: 290\n",
      "Episode: 166, Reward: 10.0, Epsilon: 0.4351, Steps: 283\n",
      "Episode: 167, Reward: 13.0, Epsilon: 0.4330, Steps: 264\n",
      "Target network updated at step 48000\n",
      "Episode: 168, Reward: 20.0, Epsilon: 0.4308, Steps: 278\n",
      "Episode: 169, Reward: 12.0, Epsilon: 0.4286, Steps: 284\n",
      "Episode: 170, Reward: 11.0, Epsilon: 0.4265, Steps: 268\n",
      "Episode: 171, Reward: 20.0, Epsilon: 0.4244, Steps: 334\n",
      "Target network updated at step 49000\n",
      "Episode: 172, Reward: 20.0, Epsilon: 0.4223, Steps: 392\n",
      "Episode: 173, Reward: 12.0, Epsilon: 0.4201, Steps: 270\n",
      "Episode: 174, Reward: 15.0, Epsilon: 0.4180, Steps: 306\n",
      "Target network updated at step 50000\n",
      "Episode: 175, Reward: 11.0, Epsilon: 0.4159, Steps: 310\n",
      "Episode: 176, Reward: 16.0, Epsilon: 0.4139, Steps: 319\n",
      "Episode: 177, Reward: 18.0, Epsilon: 0.4118, Steps: 320\n",
      "Target network updated at step 51000\n",
      "Episode: 178, Reward: 12.0, Epsilon: 0.4097, Steps: 289\n",
      "Episode: 179, Reward: 17.0, Epsilon: 0.4077, Steps: 260\n",
      "Episode: 180, Reward: 18.0, Epsilon: 0.4057, Steps: 367\n",
      "Target network updated at step 52000\n",
      "Episode: 181, Reward: 10.0, Epsilon: 0.4036, Steps: 239\n",
      "Episode: 182, Reward: 12.0, Epsilon: 0.4016, Steps: 240\n",
      "Episode: 183, Reward: 18.0, Epsilon: 0.3996, Steps: 319\n",
      "Episode: 184, Reward: 9.0, Epsilon: 0.3976, Steps: 228\n",
      "Target network updated at step 53000\n",
      "Episode: 185, Reward: 8.0, Epsilon: 0.3956, Steps: 215\n",
      "Episode: 186, Reward: 15.0, Epsilon: 0.3936, Steps: 313\n",
      "Episode: 187, Reward: 17.0, Epsilon: 0.3917, Steps: 292\n",
      "Episode: 188, Reward: 12.0, Epsilon: 0.3897, Steps: 289\n",
      "Target network updated at step 54000\n",
      "Episode: 189, Reward: 13.0, Epsilon: 0.3878, Steps: 252\n",
      "Episode: 190, Reward: 10.0, Epsilon: 0.3858, Steps: 289\n",
      "Episode: 191, Reward: 16.0, Epsilon: 0.3839, Steps: 355\n",
      "Target network updated at step 55000\n",
      "Episode: 192, Reward: 10.0, Epsilon: 0.3820, Steps: 307\n",
      "Episode: 193, Reward: 9.0, Epsilon: 0.3801, Steps: 209\n",
      "Episode: 194, Reward: 16.0, Epsilon: 0.3782, Steps: 289\n",
      "Target network updated at step 56000\n",
      "Episode: 195, Reward: 16.0, Epsilon: 0.3763, Steps: 398\n",
      "Episode: 196, Reward: 10.0, Epsilon: 0.3744, Steps: 252\n",
      "Episode: 197, Reward: 17.0, Epsilon: 0.3725, Steps: 368\n",
      "Target network updated at step 57000\n",
      "Episode: 198, Reward: 18.0, Epsilon: 0.3707, Steps: 372\n",
      "Episode: 199, Reward: 13.0, Epsilon: 0.3688, Steps: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200, Reward: 15.0, Epsilon: 0.3670, Steps: 282\n",
      "Model saved at episode 200\n",
      "Episode: 201, Reward: 16.0, Epsilon: 0.3651, Steps: 303\n",
      "Target network updated at step 58000\n",
      "Episode: 202, Reward: 11.0, Epsilon: 0.3633, Steps: 321\n",
      "Episode: 203, Reward: 17.0, Epsilon: 0.3615, Steps: 265\n",
      "Episode: 204, Reward: 20.0, Epsilon: 0.3597, Steps: 319\n",
      "Target network updated at step 59000\n",
      "Episode: 205, Reward: 19.0, Epsilon: 0.3579, Steps: 305\n",
      "Episode: 206, Reward: 19.0, Epsilon: 0.3561, Steps: 330\n",
      "Episode: 207, Reward: 12.0, Epsilon: 0.3543, Steps: 232\n",
      "Episode: 208, Reward: 12.0, Epsilon: 0.3525, Steps: 302\n",
      "Target network updated at step 60000\n",
      "Episode: 209, Reward: 17.0, Epsilon: 0.3508, Steps: 323\n",
      "Episode: 210, Reward: 18.0, Epsilon: 0.3490, Steps: 297\n",
      "Episode: 211, Reward: 10.0, Epsilon: 0.3473, Steps: 251\n",
      "Target network updated at step 61000\n",
      "Episode: 212, Reward: 9.0, Epsilon: 0.3455, Steps: 315\n",
      "Episode: 213, Reward: 12.0, Epsilon: 0.3438, Steps: 270\n",
      "Episode: 214, Reward: 18.0, Epsilon: 0.3421, Steps: 291\n",
      "Episode: 215, Reward: 12.0, Epsilon: 0.3404, Steps: 297\n",
      "Target network updated at step 62000\n",
      "Episode: 216, Reward: 9.0, Epsilon: 0.3387, Steps: 232\n",
      "Episode: 217, Reward: 10.0, Epsilon: 0.3370, Steps: 230\n",
      "Episode: 218, Reward: 9.0, Epsilon: 0.3353, Steps: 217\n",
      "Episode: 219, Reward: 16.0, Epsilon: 0.3336, Steps: 317\n",
      "Target network updated at step 63000\n",
      "Episode: 220, Reward: 12.0, Epsilon: 0.3320, Steps: 271\n",
      "Episode: 221, Reward: 14.0, Epsilon: 0.3303, Steps: 274\n",
      "Episode: 222, Reward: 18.0, Epsilon: 0.3286, Steps: 379\n",
      "Target network updated at step 64000\n",
      "Episode: 223, Reward: 8.0, Epsilon: 0.3270, Steps: 215\n",
      "Episode: 224, Reward: 13.0, Epsilon: 0.3254, Steps: 284\n",
      "Episode: 225, Reward: 9.0, Epsilon: 0.3237, Steps: 216\n",
      "Target network updated at step 65000\n",
      "Episode: 226, Reward: 16.0, Epsilon: 0.3221, Steps: 428\n",
      "Episode: 227, Reward: 15.0, Epsilon: 0.3205, Steps: 303\n",
      "Episode: 228, Reward: 19.0, Epsilon: 0.3189, Steps: 310\n",
      "Episode: 229, Reward: 13.0, Epsilon: 0.3173, Steps: 279\n",
      "Target network updated at step 66000\n",
      "Episode: 230, Reward: 13.0, Epsilon: 0.3157, Steps: 361\n",
      "Episode: 231, Reward: 15.0, Epsilon: 0.3141, Steps: 280\n",
      "Episode: 232, Reward: 16.0, Epsilon: 0.3126, Steps: 258\n",
      "Target network updated at step 67000\n",
      "Episode: 233, Reward: 14.0, Epsilon: 0.3110, Steps: 248\n",
      "Episode: 234, Reward: 14.0, Epsilon: 0.3095, Steps: 283\n",
      "Episode: 235, Reward: 13.0, Epsilon: 0.3079, Steps: 269\n",
      "Episode: 236, Reward: 17.0, Epsilon: 0.3064, Steps: 336\n",
      "Target network updated at step 68000\n",
      "Episode: 237, Reward: 14.0, Epsilon: 0.3048, Steps: 292\n",
      "Episode: 238, Reward: 10.0, Epsilon: 0.3033, Steps: 345\n",
      "Episode: 239, Reward: 8.0, Epsilon: 0.3018, Steps: 223\n",
      "Target network updated at step 69000\n",
      "Episode: 240, Reward: 12.0, Epsilon: 0.3003, Steps: 225\n",
      "Episode: 241, Reward: 13.0, Epsilon: 0.2988, Steps: 237\n",
      "Episode: 242, Reward: 16.0, Epsilon: 0.2973, Steps: 299\n",
      "Episode: 243, Reward: 19.0, Epsilon: 0.2958, Steps: 362\n",
      "Target network updated at step 70000\n",
      "Episode: 244, Reward: 14.0, Epsilon: 0.2943, Steps: 298\n",
      "Episode: 245, Reward: 13.0, Epsilon: 0.2929, Steps: 290\n",
      "Episode: 246, Reward: 14.0, Epsilon: 0.2914, Steps: 284\n",
      "Target network updated at step 71000\n",
      "Episode: 247, Reward: 15.0, Epsilon: 0.2899, Steps: 279\n",
      "Episode: 248, Reward: 13.0, Epsilon: 0.2885, Steps: 417\n",
      "Episode: 249, Reward: 17.0, Epsilon: 0.2870, Steps: 299\n",
      "Target network updated at step 72000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 250, Reward: 15.0, Epsilon: 0.2856, Steps: 337\n",
      "Model saved at episode 250\n",
      "Episode: 251, Reward: 19.0, Epsilon: 0.2842, Steps: 269\n",
      "Episode: 252, Reward: 14.0, Epsilon: 0.2828, Steps: 271\n",
      "Target network updated at step 73000\n",
      "Episode: 253, Reward: 19.0, Epsilon: 0.2813, Steps: 385\n",
      "Episode: 254, Reward: 12.0, Epsilon: 0.2799, Steps: 302\n",
      "Episode: 255, Reward: 14.0, Epsilon: 0.2785, Steps: 334\n",
      "Target network updated at step 74000\n",
      "Episode: 256, Reward: 16.0, Epsilon: 0.2771, Steps: 305\n",
      "Episode: 257, Reward: 18.0, Epsilon: 0.2758, Steps: 434\n",
      "Episode: 258, Reward: 17.0, Epsilon: 0.2744, Steps: 379\n",
      "Target network updated at step 75000\n",
      "Episode: 259, Reward: 17.0, Epsilon: 0.2730, Steps: 319\n",
      "Episode: 260, Reward: 17.0, Epsilon: 0.2716, Steps: 281\n",
      "Episode: 261, Reward: 18.0, Epsilon: 0.2703, Steps: 312\n",
      "Target network updated at step 76000\n",
      "Episode: 262, Reward: 21.0, Epsilon: 0.2689, Steps: 377\n",
      "Episode: 263, Reward: 13.0, Epsilon: 0.2676, Steps: 325\n",
      "Episode: 264, Reward: 16.0, Epsilon: 0.2663, Steps: 280\n",
      "Target network updated at step 77000\n",
      "Episode: 265, Reward: 17.0, Epsilon: 0.2649, Steps: 316\n",
      "Episode: 266, Reward: 20.0, Epsilon: 0.2636, Steps: 330\n",
      "Episode: 267, Reward: 14.0, Epsilon: 0.2623, Steps: 290\n",
      "Target network updated at step 78000\n",
      "Episode: 268, Reward: 19.0, Epsilon: 0.2610, Steps: 335\n",
      "Episode: 269, Reward: 17.0, Epsilon: 0.2597, Steps: 287\n",
      "Episode: 270, Reward: 14.0, Epsilon: 0.2584, Steps: 320\n",
      "Episode: 271, Reward: 15.0, Epsilon: 0.2571, Steps: 258\n",
      "Target network updated at step 79000\n",
      "Episode: 272, Reward: 14.0, Epsilon: 0.2558, Steps: 320\n",
      "Episode: 273, Reward: 10.0, Epsilon: 0.2545, Steps: 293\n",
      "Episode: 274, Reward: 18.0, Epsilon: 0.2532, Steps: 331\n",
      "Target network updated at step 80000\n",
      "Episode: 275, Reward: 19.0, Epsilon: 0.2520, Steps: 386\n",
      "Episode: 276, Reward: 18.0, Epsilon: 0.2507, Steps: 376\n",
      "Episode: 277, Reward: 17.0, Epsilon: 0.2495, Steps: 341\n",
      "Target network updated at step 81000\n",
      "Episode: 278, Reward: 17.0, Epsilon: 0.2482, Steps: 281\n",
      "Episode: 279, Reward: 13.0, Epsilon: 0.2470, Steps: 472\n",
      "Episode: 280, Reward: 10.0, Epsilon: 0.2457, Steps: 231\n",
      "Target network updated at step 82000\n",
      "Episode: 281, Reward: 13.0, Epsilon: 0.2445, Steps: 277\n",
      "Episode: 282, Reward: 13.0, Epsilon: 0.2433, Steps: 249\n",
      "Episode: 283, Reward: 13.0, Epsilon: 0.2421, Steps: 395\n",
      "Target network updated at step 83000\n",
      "Episode: 284, Reward: 14.0, Epsilon: 0.2409, Steps: 284\n",
      "Episode: 285, Reward: 11.0, Epsilon: 0.2397, Steps: 268\n",
      "Episode: 286, Reward: 22.0, Epsilon: 0.2385, Steps: 319\n",
      "Target network updated at step 84000\n",
      "Episode: 287, Reward: 21.0, Epsilon: 0.2373, Steps: 315\n",
      "Episode: 288, Reward: 20.0, Epsilon: 0.2361, Steps: 361\n",
      "Episode: 289, Reward: 14.0, Epsilon: 0.2349, Steps: 303\n",
      "Episode: 290, Reward: 19.0, Epsilon: 0.2337, Steps: 306\n",
      "Target network updated at step 85000\n",
      "Episode: 291, Reward: 14.0, Epsilon: 0.2326, Steps: 346\n",
      "Episode: 292, Reward: 13.0, Epsilon: 0.2314, Steps: 273\n",
      "Episode: 293, Reward: 16.0, Epsilon: 0.2302, Steps: 377\n",
      "Target network updated at step 86000\n",
      "Episode: 294, Reward: 15.0, Epsilon: 0.2291, Steps: 265\n",
      "Episode: 295, Reward: 15.0, Epsilon: 0.2279, Steps: 317\n",
      "Episode: 296, Reward: 14.0, Epsilon: 0.2268, Steps: 292\n",
      "Target network updated at step 87000\n",
      "Episode: 297, Reward: 12.0, Epsilon: 0.2257, Steps: 289\n",
      "Episode: 298, Reward: 15.0, Epsilon: 0.2245, Steps: 281\n",
      "Episode: 299, Reward: 18.0, Epsilon: 0.2234, Steps: 260\n",
      "Target network updated at step 88000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300, Reward: 19.0, Epsilon: 0.2223, Steps: 380\n",
      "Model saved at episode 300\n",
      "Episode: 301, Reward: 12.0, Epsilon: 0.2212, Steps: 393\n",
      "Episode: 302, Reward: 15.0, Epsilon: 0.2201, Steps: 384\n",
      "Target network updated at step 89000\n",
      "Episode: 303, Reward: 16.0, Epsilon: 0.2190, Steps: 299\n",
      "Episode: 304, Reward: 15.0, Epsilon: 0.2179, Steps: 281\n",
      "Episode: 305, Reward: 19.0, Epsilon: 0.2168, Steps: 396\n",
      "Target network updated at step 90000\n",
      "Episode: 306, Reward: 13.0, Epsilon: 0.2157, Steps: 260\n",
      "Episode: 307, Reward: 13.0, Epsilon: 0.2146, Steps: 273\n",
      "Episode: 308, Reward: 20.0, Epsilon: 0.2136, Steps: 420\n",
      "Target network updated at step 91000\n",
      "Episode: 309, Reward: 16.0, Epsilon: 0.2125, Steps: 344\n",
      "Episode: 310, Reward: 14.0, Epsilon: 0.2114, Steps: 319\n",
      "Episode: 311, Reward: 18.0, Epsilon: 0.2104, Steps: 361\n",
      "Target network updated at step 92000\n",
      "Episode: 312, Reward: 17.0, Epsilon: 0.2093, Steps: 387\n",
      "Episode: 313, Reward: 12.0, Epsilon: 0.2083, Steps: 334\n",
      "Episode: 314, Reward: 18.0, Epsilon: 0.2072, Steps: 300\n",
      "Target network updated at step 93000\n",
      "Episode: 315, Reward: 17.0, Epsilon: 0.2062, Steps: 321\n",
      "Episode: 316, Reward: 21.0, Epsilon: 0.2052, Steps: 481\n",
      "Episode: 317, Reward: 16.0, Epsilon: 0.2041, Steps: 313\n",
      "Target network updated at step 94000\n",
      "Episode: 318, Reward: 19.0, Epsilon: 0.2031, Steps: 329\n",
      "Episode: 319, Reward: 22.0, Epsilon: 0.2021, Steps: 332\n",
      "Episode: 320, Reward: 11.0, Epsilon: 0.2011, Steps: 281\n",
      "Target network updated at step 95000\n",
      "Episode: 321, Reward: 18.0, Epsilon: 0.2001, Steps: 350\n",
      "Episode: 322, Reward: 10.0, Epsilon: 0.1991, Steps: 286\n",
      "Episode: 323, Reward: 11.0, Epsilon: 0.1981, Steps: 230\n",
      "Target network updated at step 96000\n",
      "Episode: 324, Reward: 9.0, Epsilon: 0.1971, Steps: 289\n",
      "Episode: 325, Reward: 16.0, Epsilon: 0.1961, Steps: 321\n",
      "Episode: 326, Reward: 16.0, Epsilon: 0.1951, Steps: 321\n",
      "Target network updated at step 97000\n",
      "Episode: 327, Reward: 23.0, Epsilon: 0.1942, Steps: 340\n",
      "Episode: 328, Reward: 13.0, Epsilon: 0.1932, Steps: 305\n",
      "Episode: 329, Reward: 17.0, Epsilon: 0.1922, Steps: 355\n",
      "Episode: 330, Reward: 14.0, Epsilon: 0.1913, Steps: 234\n",
      "Target network updated at step 98000\n",
      "Episode: 331, Reward: 17.0, Epsilon: 0.1903, Steps: 293\n",
      "Episode: 332, Reward: 19.0, Epsilon: 0.1893, Steps: 322\n",
      "Episode: 333, Reward: 21.0, Epsilon: 0.1884, Steps: 314\n",
      "Target network updated at step 99000\n",
      "Episode: 334, Reward: 19.0, Epsilon: 0.1875, Steps: 326\n",
      "Episode: 335, Reward: 18.0, Epsilon: 0.1865, Steps: 297\n",
      "Episode: 336, Reward: 14.0, Epsilon: 0.1856, Steps: 298\n",
      "Target network updated at step 100000\n",
      "Episode: 337, Reward: 22.0, Epsilon: 0.1847, Steps: 330\n",
      "Episode: 338, Reward: 20.0, Epsilon: 0.1837, Steps: 448\n",
      "Episode: 339, Reward: 18.0, Epsilon: 0.1828, Steps: 343\n",
      "Target network updated at step 101000\n",
      "Episode: 340, Reward: 18.0, Epsilon: 0.1819, Steps: 294\n",
      "Episode: 341, Reward: 19.0, Epsilon: 0.1810, Steps: 290\n",
      "Episode: 342, Reward: 11.0, Epsilon: 0.1801, Steps: 230\n",
      "Episode: 343, Reward: 15.0, Epsilon: 0.1792, Steps: 272\n",
      "Target network updated at step 102000\n",
      "Episode: 344, Reward: 14.0, Epsilon: 0.1783, Steps: 310\n",
      "Episode: 345, Reward: 9.0, Epsilon: 0.1774, Steps: 273\n",
      "Episode: 346, Reward: 14.0, Epsilon: 0.1765, Steps: 314\n",
      "Target network updated at step 103000\n",
      "Episode: 347, Reward: 20.0, Epsilon: 0.1756, Steps: 302\n",
      "Episode: 348, Reward: 16.0, Epsilon: 0.1748, Steps: 270\n",
      "Episode: 349, Reward: 13.0, Epsilon: 0.1739, Steps: 466\n",
      "Target network updated at step 104000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 350, Reward: 17.0, Epsilon: 0.1730, Steps: 390\n",
      "Model saved at episode 350\n",
      "Episode: 351, Reward: 20.0, Epsilon: 0.1721, Steps: 341\n",
      "Target network updated at step 105000\n",
      "Episode: 352, Reward: 12.0, Epsilon: 0.1713, Steps: 399\n",
      "Episode: 353, Reward: 11.0, Epsilon: 0.1704, Steps: 278\n",
      "Episode: 354, Reward: 18.0, Epsilon: 0.1696, Steps: 396\n",
      "Episode: 355, Reward: 16.0, Epsilon: 0.1687, Steps: 286\n",
      "Target network updated at step 106000\n",
      "Episode: 356, Reward: 11.0, Epsilon: 0.1679, Steps: 317\n",
      "Episode: 357, Reward: 14.0, Epsilon: 0.1670, Steps: 313\n",
      "Episode: 358, Reward: 19.0, Epsilon: 0.1662, Steps: 333\n",
      "Target network updated at step 107000\n",
      "Episode: 359, Reward: 19.0, Epsilon: 0.1654, Steps: 285\n",
      "Episode: 360, Reward: 19.0, Epsilon: 0.1646, Steps: 258\n",
      "Episode: 361, Reward: 17.0, Epsilon: 0.1637, Steps: 299\n",
      "Target network updated at step 108000\n",
      "Episode: 362, Reward: 17.0, Epsilon: 0.1629, Steps: 228\n",
      "Episode: 363, Reward: 17.0, Epsilon: 0.1621, Steps: 344\n",
      "Episode: 364, Reward: 13.0, Epsilon: 0.1613, Steps: 270\n",
      "Episode: 365, Reward: 14.0, Epsilon: 0.1605, Steps: 267\n",
      "Target network updated at step 109000\n",
      "Episode: 366, Reward: 21.0, Epsilon: 0.1597, Steps: 319\n",
      "Episode: 367, Reward: 17.0, Epsilon: 0.1589, Steps: 265\n",
      "Episode: 368, Reward: 12.0, Epsilon: 0.1581, Steps: 288\n",
      "Episode: 369, Reward: 9.0, Epsilon: 0.1573, Steps: 221\n",
      "Target network updated at step 110000\n",
      "Episode: 370, Reward: 14.0, Epsilon: 0.1565, Steps: 326\n",
      "Episode: 371, Reward: 12.0, Epsilon: 0.1557, Steps: 298\n",
      "Episode: 372, Reward: 12.0, Epsilon: 0.1549, Steps: 258\n",
      "Target network updated at step 111000\n",
      "Episode: 373, Reward: 14.0, Epsilon: 0.1542, Steps: 248\n",
      "Episode: 374, Reward: 12.0, Epsilon: 0.1534, Steps: 240\n",
      "Episode: 375, Reward: 15.0, Epsilon: 0.1526, Steps: 283\n",
      "Target network updated at step 112000\n",
      "Episode: 376, Reward: 15.0, Epsilon: 0.1519, Steps: 445\n",
      "Episode: 377, Reward: 20.0, Epsilon: 0.1511, Steps: 337\n",
      "Episode: 378, Reward: 18.0, Epsilon: 0.1504, Steps: 308\n",
      "Target network updated at step 113000\n",
      "Episode: 379, Reward: 17.0, Epsilon: 0.1496, Steps: 338\n",
      "Episode: 380, Reward: 11.0, Epsilon: 0.1489, Steps: 229\n",
      "Episode: 381, Reward: 11.0, Epsilon: 0.1481, Steps: 217\n",
      "Episode: 382, Reward: 19.0, Epsilon: 0.1474, Steps: 262\n",
      "Target network updated at step 114000\n",
      "Episode: 383, Reward: 16.0, Epsilon: 0.1466, Steps: 266\n",
      "Episode: 384, Reward: 13.0, Epsilon: 0.1459, Steps: 308\n",
      "Episode: 385, Reward: 14.0, Epsilon: 0.1452, Steps: 288\n",
      "Episode: 386, Reward: 19.0, Epsilon: 0.1444, Steps: 321\n",
      "Target network updated at step 115000\n",
      "Episode: 387, Reward: 9.0, Epsilon: 0.1437, Steps: 227\n",
      "Episode: 388, Reward: 15.0, Epsilon: 0.1430, Steps: 267\n",
      "Episode: 389, Reward: 16.0, Epsilon: 0.1423, Steps: 305\n",
      "Target network updated at step 116000\n",
      "Episode: 390, Reward: 14.0, Epsilon: 0.1416, Steps: 337\n",
      "Episode: 391, Reward: 12.0, Epsilon: 0.1409, Steps: 277\n",
      "Episode: 392, Reward: 20.0, Epsilon: 0.1402, Steps: 361\n",
      "Target network updated at step 117000\n",
      "Episode: 393, Reward: 15.0, Epsilon: 0.1395, Steps: 279\n",
      "Episode: 394, Reward: 15.0, Epsilon: 0.1388, Steps: 278\n",
      "Episode: 395, Reward: 17.0, Epsilon: 0.1381, Steps: 553\n",
      "Target network updated at step 118000\n",
      "Episode: 396, Reward: 18.0, Epsilon: 0.1374, Steps: 234\n",
      "Episode: 397, Reward: 17.0, Epsilon: 0.1367, Steps: 425\n",
      "Episode: 398, Reward: 15.0, Epsilon: 0.1360, Steps: 305\n",
      "Target network updated at step 119000\n",
      "Episode: 399, Reward: 10.0, Epsilon: 0.1353, Steps: 222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 400, Reward: 13.0, Epsilon: 0.1347, Steps: 284\n",
      "Model saved at episode 400\n",
      "Episode: 401, Reward: 19.0, Epsilon: 0.1340, Steps: 281\n",
      "Episode: 402, Reward: 19.0, Epsilon: 0.1333, Steps: 279\n",
      "Target network updated at step 120000\n",
      "Episode: 403, Reward: 10.0, Epsilon: 0.1326, Steps: 233\n",
      "Episode: 404, Reward: 16.0, Epsilon: 0.1320, Steps: 295\n",
      "Episode: 405, Reward: 16.0, Epsilon: 0.1313, Steps: 379\n",
      "Target network updated at step 121000\n",
      "Episode: 406, Reward: 10.0, Epsilon: 0.1307, Steps: 233\n",
      "Episode: 407, Reward: 13.0, Epsilon: 0.1300, Steps: 394\n",
      "Episode: 408, Reward: 22.0, Epsilon: 0.1294, Steps: 362\n",
      "Target network updated at step 122000\n",
      "Episode: 409, Reward: 13.0, Epsilon: 0.1287, Steps: 281\n",
      "Episode: 410, Reward: 14.0, Epsilon: 0.1281, Steps: 424\n",
      "Episode: 411, Reward: 19.0, Epsilon: 0.1274, Steps: 289\n",
      "Target network updated at step 123000\n",
      "Episode: 412, Reward: 23.0, Epsilon: 0.1268, Steps: 415\n",
      "Episode: 413, Reward: 10.0, Epsilon: 0.1262, Steps: 272\n",
      "Episode: 414, Reward: 15.0, Epsilon: 0.1255, Steps: 344\n",
      "Target network updated at step 124000\n",
      "Episode: 415, Reward: 18.0, Epsilon: 0.1249, Steps: 393\n",
      "Episode: 416, Reward: 17.0, Epsilon: 0.1243, Steps: 280\n",
      "Episode: 417, Reward: 20.0, Epsilon: 0.1237, Steps: 267\n",
      "Target network updated at step 125000\n",
      "Episode: 418, Reward: 14.0, Epsilon: 0.1230, Steps: 357\n",
      "Episode: 419, Reward: 17.0, Epsilon: 0.1224, Steps: 342\n",
      "Episode: 420, Reward: 17.0, Epsilon: 0.1218, Steps: 336\n",
      "Target network updated at step 126000\n",
      "Episode: 421, Reward: 18.0, Epsilon: 0.1212, Steps: 391\n",
      "Episode: 422, Reward: 12.0, Epsilon: 0.1206, Steps: 272\n",
      "Episode: 423, Reward: 14.0, Epsilon: 0.1200, Steps: 285\n",
      "Target network updated at step 127000\n",
      "Episode: 424, Reward: 19.0, Epsilon: 0.1194, Steps: 401\n",
      "Episode: 425, Reward: 17.0, Epsilon: 0.1188, Steps: 480\n",
      "Target network updated at step 128000\n",
      "Episode: 426, Reward: 16.0, Epsilon: 0.1182, Steps: 466\n",
      "Episode: 427, Reward: 21.0, Epsilon: 0.1176, Steps: 389\n",
      "Episode: 428, Reward: 15.0, Epsilon: 0.1170, Steps: 337\n",
      "Target network updated at step 129000\n",
      "Episode: 429, Reward: 20.0, Epsilon: 0.1164, Steps: 342\n",
      "Episode: 430, Reward: 10.0, Epsilon: 0.1159, Steps: 349\n",
      "Episode: 431, Reward: 17.0, Epsilon: 0.1153, Steps: 361\n",
      "Target network updated at step 130000\n",
      "Episode: 432, Reward: 19.0, Epsilon: 0.1147, Steps: 312\n",
      "Episode: 433, Reward: 17.0, Epsilon: 0.1141, Steps: 301\n",
      "Episode: 434, Reward: 14.0, Epsilon: 0.1136, Steps: 443\n",
      "Target network updated at step 131000\n",
      "Episode: 435, Reward: 12.0, Epsilon: 0.1130, Steps: 273\n",
      "Episode: 436, Reward: 17.0, Epsilon: 0.1124, Steps: 281\n",
      "Episode: 437, Reward: 18.0, Epsilon: 0.1119, Steps: 305\n",
      "Target network updated at step 132000\n",
      "Episode: 438, Reward: 13.0, Epsilon: 0.1113, Steps: 333\n",
      "Episode: 439, Reward: 19.0, Epsilon: 0.1107, Steps: 333\n",
      "Episode: 440, Reward: 9.0, Epsilon: 0.1102, Steps: 313\n",
      "Target network updated at step 133000\n",
      "Episode: 441, Reward: 21.0, Epsilon: 0.1096, Steps: 485\n",
      "Episode: 442, Reward: 20.0, Epsilon: 0.1091, Steps: 321\n",
      "Episode: 443, Reward: 19.0, Epsilon: 0.1085, Steps: 406\n",
      "Target network updated at step 134000\n",
      "Episode: 444, Reward: 16.0, Epsilon: 0.1080, Steps: 280\n",
      "Episode: 445, Reward: 20.0, Epsilon: 0.1075, Steps: 379\n",
      "Target network updated at step 135000\n",
      "Episode: 446, Reward: 17.0, Epsilon: 0.1069, Steps: 605\n",
      "Episode: 447, Reward: 18.0, Epsilon: 0.1064, Steps: 478\n",
      "Target network updated at step 136000\n",
      "Episode: 448, Reward: 18.0, Epsilon: 0.1059, Steps: 404\n",
      "Episode: 449, Reward: 13.0, Epsilon: 0.1053, Steps: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 450, Reward: 14.0, Epsilon: 0.1048, Steps: 281\n",
      "Model saved at episode 450\n",
      "Target network updated at step 137000\n",
      "Episode: 451, Reward: 17.0, Epsilon: 0.1043, Steps: 486\n",
      "Episode: 452, Reward: 18.0, Epsilon: 0.1038, Steps: 252\n",
      "Episode: 453, Reward: 20.0, Epsilon: 0.1032, Steps: 318\n",
      "Target network updated at step 138000\n",
      "Episode: 454, Reward: 21.0, Epsilon: 0.1027, Steps: 329\n",
      "Episode: 455, Reward: 19.0, Epsilon: 0.1022, Steps: 447\n",
      "Episode: 456, Reward: 14.0, Epsilon: 0.1017, Steps: 377\n",
      "Target network updated at step 139000\n",
      "Episode: 457, Reward: 20.0, Epsilon: 0.1012, Steps: 305\n",
      "Episode: 458, Reward: 16.0, Epsilon: 0.1007, Steps: 496\n",
      "Target network updated at step 140000\n",
      "Episode: 459, Reward: 16.0, Epsilon: 0.1002, Steps: 394\n",
      "Episode: 460, Reward: 18.0, Epsilon: 0.0997, Steps: 384\n",
      "Episode: 461, Reward: 16.0, Epsilon: 0.0992, Steps: 391\n",
      "Target network updated at step 141000\n",
      "Episode: 462, Reward: 15.0, Epsilon: 0.0987, Steps: 414\n",
      "Episode: 463, Reward: 16.0, Epsilon: 0.0982, Steps: 439\n",
      "Target network updated at step 142000\n",
      "Episode: 464, Reward: 15.0, Epsilon: 0.0977, Steps: 301\n",
      "Episode: 465, Reward: 16.0, Epsilon: 0.0972, Steps: 314\n",
      "Episode: 466, Reward: 17.0, Epsilon: 0.0967, Steps: 234\n",
      "Episode: 467, Reward: 9.0, Epsilon: 0.0962, Steps: 240\n",
      "Target network updated at step 143000\n",
      "Episode: 468, Reward: 16.0, Epsilon: 0.0958, Steps: 294\n",
      "Episode: 469, Reward: 12.0, Epsilon: 0.0953, Steps: 562\n",
      "Episode: 470, Reward: 16.0, Epsilon: 0.0948, Steps: 288\n",
      "Target network updated at step 144000\n",
      "Episode: 471, Reward: 17.0, Epsilon: 0.0943, Steps: 378\n",
      "Episode: 472, Reward: 19.0, Epsilon: 0.0939, Steps: 393\n",
      "Target network updated at step 145000\n",
      "Episode: 473, Reward: 16.0, Epsilon: 0.0934, Steps: 433\n",
      "Episode: 474, Reward: 16.0, Epsilon: 0.0929, Steps: 709\n",
      "Target network updated at step 146000\n",
      "Episode: 475, Reward: 18.0, Epsilon: 0.0925, Steps: 285\n",
      "Episode: 476, Reward: 24.0, Epsilon: 0.0920, Steps: 653\n",
      "Target network updated at step 147000\n",
      "Episode: 477, Reward: 13.0, Epsilon: 0.0915, Steps: 271\n",
      "Episode: 478, Reward: 18.0, Epsilon: 0.0911, Steps: 554\n",
      "Target network updated at step 148000\n",
      "Episode: 479, Reward: 18.0, Epsilon: 0.0906, Steps: 478\n",
      "Episode: 480, Reward: 19.0, Epsilon: 0.0902, Steps: 327\n",
      "Target network updated at step 149000\n",
      "Episode: 481, Reward: 15.0, Epsilon: 0.0897, Steps: 632\n",
      "Episode: 482, Reward: 17.0, Epsilon: 0.0893, Steps: 616\n",
      "Target network updated at step 150000\n",
      "Episode: 483, Reward: 17.0, Epsilon: 0.0888, Steps: 565\n",
      "Episode: 484, Reward: 17.0, Epsilon: 0.0884, Steps: 293\n",
      "Episode: 485, Reward: 9.0, Epsilon: 0.0879, Steps: 332\n",
      "Target network updated at step 151000\n",
      "Episode: 486, Reward: 15.0, Epsilon: 0.0875, Steps: 459\n",
      "Episode: 487, Reward: 17.0, Epsilon: 0.0871, Steps: 537\n",
      "Target network updated at step 152000\n",
      "Episode: 488, Reward: 12.0, Epsilon: 0.0866, Steps: 370\n",
      "Episode: 489, Reward: 18.0, Epsilon: 0.0862, Steps: 388\n",
      "Episode: 490, Reward: 18.0, Epsilon: 0.0858, Steps: 282\n",
      "Target network updated at step 153000\n",
      "Episode: 491, Reward: 17.0, Epsilon: 0.0853, Steps: 283\n",
      "Episode: 492, Reward: 17.0, Epsilon: 0.0849, Steps: 306\n",
      "Episode: 493, Reward: 14.0, Epsilon: 0.0845, Steps: 328\n",
      "Target network updated at step 154000\n",
      "Episode: 494, Reward: 15.0, Epsilon: 0.0841, Steps: 321\n",
      "Episode: 495, Reward: 18.0, Epsilon: 0.0836, Steps: 389\n",
      "Episode: 496, Reward: 9.0, Epsilon: 0.0832, Steps: 228\n",
      "Target network updated at step 155000\n",
      "Episode: 497, Reward: 17.0, Epsilon: 0.0828, Steps: 448\n",
      "Episode: 498, Reward: 12.0, Epsilon: 0.0824, Steps: 274\n",
      "Episode: 499, Reward: 18.0, Epsilon: 0.0820, Steps: 286\n",
      "Target network updated at step 156000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500, Reward: 18.0, Epsilon: 0.0816, Steps: 305\n",
      "Model saved at episode 500\n",
      "Episode: 501, Reward: 16.0, Epsilon: 0.0812, Steps: 369\n",
      "Episode: 502, Reward: 20.0, Epsilon: 0.0808, Steps: 302\n",
      "Target network updated at step 157000\n",
      "Episode: 503, Reward: 15.0, Epsilon: 0.0804, Steps: 735\n",
      "Episode: 504, Reward: 18.0, Epsilon: 0.0800, Steps: 276\n",
      "Target network updated at step 158000\n",
      "Episode: 505, Reward: 17.0, Epsilon: 0.0796, Steps: 713\n",
      "Target network updated at step 159000\n",
      "Episode: 506, Reward: 23.0, Epsilon: 0.0792, Steps: 793\n",
      "Episode: 507, Reward: 13.0, Epsilon: 0.0788, Steps: 395\n",
      "Episode: 508, Reward: 15.0, Epsilon: 0.0784, Steps: 318\n",
      "Target network updated at step 160000\n",
      "Episode: 509, Reward: 12.0, Epsilon: 0.0780, Steps: 312\n",
      "Episode: 510, Reward: 18.0, Epsilon: 0.0776, Steps: 320\n",
      "Episode: 511, Reward: 8.0, Epsilon: 0.0772, Steps: 306\n",
      "Target network updated at step 161000\n",
      "Episode: 512, Reward: 22.0, Epsilon: 0.0768, Steps: 705\n",
      "Target network updated at step 162000\n",
      "Episode: 513, Reward: 18.0, Epsilon: 0.0764, Steps: 717\n",
      "Episode: 514, Reward: 11.0, Epsilon: 0.0760, Steps: 363\n",
      "Episode: 515, Reward: 12.0, Epsilon: 0.0757, Steps: 271\n",
      "Target network updated at step 163000\n",
      "Episode: 516, Reward: 14.0, Epsilon: 0.0753, Steps: 342\n",
      "Target network updated at step 164000\n",
      "Episode: 517, Reward: 19.0, Epsilon: 0.0749, Steps: 701\n",
      "Episode: 518, Reward: 11.0, Epsilon: 0.0745, Steps: 236\n",
      "Episode: 519, Reward: 18.0, Epsilon: 0.0742, Steps: 410\n",
      "Target network updated at step 165000\n",
      "Episode: 520, Reward: 19.0, Epsilon: 0.0738, Steps: 554\n",
      "Episode: 521, Reward: 21.0, Epsilon: 0.0734, Steps: 389\n",
      "Target network updated at step 166000\n",
      "Episode: 522, Reward: 13.0, Epsilon: 0.0731, Steps: 422\n",
      "Episode: 523, Reward: 19.0, Epsilon: 0.0727, Steps: 244\n",
      "Episode: 524, Reward: 18.0, Epsilon: 0.0723, Steps: 342\n",
      "Episode: 525, Reward: 13.0, Epsilon: 0.0720, Steps: 321\n",
      "Target network updated at step 167000\n",
      "Episode: 526, Reward: 13.0, Epsilon: 0.0716, Steps: 735\n",
      "Episode: 527, Reward: 14.0, Epsilon: 0.0712, Steps: 319\n",
      "Target network updated at step 168000\n",
      "Episode: 528, Reward: 10.0, Epsilon: 0.0709, Steps: 226\n",
      "Episode: 529, Reward: 16.0, Epsilon: 0.0705, Steps: 332\n",
      "Episode: 530, Reward: 11.0, Epsilon: 0.0702, Steps: 235\n",
      "Target network updated at step 169000\n",
      "Episode: 531, Reward: 10.0, Epsilon: 0.0698, Steps: 236\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # You can adjust these parameters as needed\n",
    "    train_dqn(episodes=1000,\n",
    "              max_steps=10000, \n",
    "              batch_size=32, \n",
    "              gamma=0.99, \n",
    "              epsilon_start=1.0, \n",
    "              epsilon_end=0.01, \n",
    "              epsilon_decay=0.995,\n",
    "              update_target_freq=1000,\n",
    "              memory_capacity=50000,\n",
    "              save_freq=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
